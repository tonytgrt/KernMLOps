{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da464b6-89fc-4ca2-9290-f3bebbe2dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make docker-image > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62bef45-339e-44c1-8309-874c236ed23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CONTAINER_CMD=\"bash -lc 'make install-ycsb\" make docker > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a1beeb-3882-4dd0-91c7-aa98c303c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pexpect\n",
    "import os\n",
    "import time\n",
    "\n",
    "\"\"\" Collector class has helper methods to interact with kermit\"\"\"\n",
    "class Collector: \n",
    "    def __init__(self, config: Path):\n",
    "        self.env = os.environ.copy()\n",
    "        self.env[\"INTERACTIVE\"] = \"it\"\n",
    "        self.env[\"CONTAINER_CMD\"] = f\"bash -lc 'KERNMLOPS_CONFIG_FILE={config} make collect-data'\"\n",
    "        self.collect_process : pexpect.spawn | None = None\n",
    "\n",
    "    def start_collection(self, logfile=None):\n",
    "        self.collect_process = pexpect.spawn(\"make docker\", env=self.env, timeout=None, logfile=logfile)\n",
    "        self.collect_process.expect_exact([\"Started benchmark\"])\n",
    "\n",
    "    def _after_run_generate_file_data() -> dict[str, list[Path]]:\n",
    "        start_path : Path = Path(\"./data\")\n",
    "        list_of_collect_id_dirs = start_path.glob(\"*/*/*\")\n",
    "        latest_collect_id = max(list_of_collect_id_dirs, key=os.path.getctime)\n",
    "        list_of_files = latest_collect_id.glob(\"*.*.parquet\")\n",
    "        output = {}\n",
    "        for f in list_of_files:\n",
    "            index = str(f).removeprefix(str(f.parent) + \"/\").split(\".\")[0]\n",
    "            if index not in output.keys():\n",
    "                output[index] = []\n",
    "            output[index].append(f)\n",
    "        return output\n",
    "        \n",
    "    def wait(self) -> int:\n",
    "        if self.collect_process is None:\n",
    "            return\n",
    "        self.collect_process.expect([pexpect.EOF])\n",
    "        self.collect_process.wait()\n",
    "        return Collector._after_run_generate_file_data()\n",
    "        \n",
    "    def stop_collection(self):\n",
    "        if self.collect_process is None:\n",
    "            return\n",
    "        self.collect_process.sendline(\"END\")\n",
    "        return self.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5874f9a-d168-446b-9a52-ec5b49f7c35f",
   "metadata": {},
   "source": [
    "There are two ways to run kermit:\n",
    "- With the raw config\n",
    "- With a pre-programmed benchmark config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419c3644-76a6-4a25-8b77-8896bd9698f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "# New TCP Collector\n",
    "collect = Collector(\"./config/raw_overrides.yaml\")\n",
    "collect.start_collection()\n",
    "\n",
    "# Generate some TCP traffic\n",
    "!nc -l 8080 &  # Listen on port 8080\n",
    "!echo \"testtesttesttesttesttesttesttesttesttest\" | nc localhost 8080 \n",
    "!echo \"testtesttesttesttesttesttesttesttesttest\" | nc localhost 8080 \n",
    "!echo \"testtesttesttesttesttesttesttesttesttest\" | nc localhost 8080 \n",
    "!echo \"testtesttesttesttesttesttesttesttesttest\" | nc localhost 8080 \n",
    "!echo \"testtesttesttesttesttesttesttesttesttest\" | nc localhost 8080 \n",
    "!echo \"testtesttesttesttesttesttesttesttesttest\" | nc localhost 8080 \n",
    "\n",
    "data = collect.stop_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c261ccd4-7fef-4d9e-ad4f-823e7d35dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d863f306-5194-4d1a-90f4-93694aeb3ca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tcp_v4_rcv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Analyze TCP branches\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tcp_df = pl.read_parquet(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtcp_v4_rcv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m      4\u001b[39m tcp_state_df = pl.read_parquet(data[\u001b[33m\"\u001b[39m\u001b[33mtcp_state_process\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(tcp_df.group_by(\u001b[33m\"\u001b[39m\u001b[33mcomm\u001b[39m\u001b[33m\"\u001b[39m).count().sort(\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m, descending=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[31mKeyError\u001b[39m: 'tcp_v4_rcv'"
     ]
    }
   ],
   "source": [
    "# Analyze TCP branches\n",
    "import polars as pl\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "tcp_state_df = pl.read_parquet(data[\"tcp_state_process\"][0])\n",
    "\n",
    "print(tcp_df.group_by(\"comm\").count().sort(\"count\", descending=True))\n",
    "print(tcp_state_df.group_by(\"comm\").count().sort(\"count\", descending=True))\n",
    "\n",
    "nc = tcp_df.filter(pl.col(\"comm\").str.contains(\"nc\"))\n",
    "nc_state = tcp_state_df.filter(pl.col(\"comm\").str.contains(\"nc\"))\n",
    "\n",
    "# Show branch distribution\n",
    "print(nc.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
    "print(nc_state.group_by(\"event_type_name\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Show drop reasons\n",
    "drops = nc.filter(pl.col(\"drop_reason\") > 0)\n",
    "print(drops.group_by(\"drop_reason_name\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0390d-a518-4e60-8d8e-d52cd2ed2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load and prepare data\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data (assuming data dict is already populated from previous cells)\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "tcp_state_df = pl.read_parquet(data[\"tcp_state_process\"][0])\n",
    "\n",
    "print(f\"Loaded {len(tcp_df):,} tcp_v4_rcv events\")\n",
    "print(f\"Loaded {len(tcp_state_df):,} tcp_state_process events\")\n",
    "\n",
    "# Cell 2: Basic exploration of TCP state data\n",
    "# Show sample of state transitions\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "print(\"\\nSample State Transitions:\")\n",
    "print(transitions.select(['comm', 'old_state_name', 'new_state_name', 'ts_uptime_us']).head(10))\n",
    "\n",
    "# Show event type distribution\n",
    "print(\"\\nEvent Type Distribution:\")\n",
    "print(tcp_state_df.group_by('event_type_name').len().sort('len', descending=True))\n",
    "\n",
    "# Cell 3: Deep dive into specific process (e.g., nc or iperf3)\n",
    "# Filter for a specific process\n",
    "process_name = \"nc\"  # or \"iperf3\" depending on your test\n",
    "process_events = tcp_state_df.filter(pl.col('comm').str.contains(process_name))\n",
    "\n",
    "if len(process_events) > 0:\n",
    "    print(f\"\\n{process_name} TCP State Analysis:\")\n",
    "    print(f\"Total events: {len(process_events)}\")\n",
    "    \n",
    "    # Show state progression\n",
    "    print(\"\\nState progression timeline:\")\n",
    "    timeline = process_events.select(['ts_uptime_us', 'event_type_name', 'old_state_name', 'new_state_name']).sort('ts_uptime_us')\n",
    "    \n",
    "    # Convert to relative time in milliseconds\n",
    "    start_time = timeline['ts_uptime_us'].min()\n",
    "    timeline_with_relative = timeline.with_columns([\n",
    "        ((pl.col('ts_uptime_us') - start_time) / 1000).alias('relative_ms')\n",
    "    ])\n",
    "    \n",
    "    # Show key events\n",
    "    for row in timeline_with_relative.head(20).iter_rows():\n",
    "        if row[1] == 'TRANSITION':\n",
    "            print(f\"{row[4]:8.1f}ms: {row[2]} → {row[3]}\")\n",
    "        else:\n",
    "            print(f\"{row[4]:8.1f}ms: {row[1]} in {row[3]}\")\n",
    "\n",
    "# Cell 4: Connection lifecycle analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONNECTION LIFECYCLE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Group events by process and analyze connection patterns\n",
    "connection_patterns = tcp_state_df.group_by('comm').agg([\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'SYN_SENT').len().alias('syn_sent_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'SYN_RECV').len().alias('syn_recv_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'ESTABLISHED').len().alias('established_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'TIME_WAIT').len().alias('time_wait_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('event_type_name') == 'ERROR').len().alias('error_count')\n",
    "]).filter(pl.col('established_count') > 0).sort('established_count', descending=True)\n",
    "\n",
    "print(\"Connection patterns by process:\")\n",
    "print(connection_patterns.head(10))\n",
    "\n",
    "# Cell 5: Visualize state machine flow\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# State transition graph\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "if len(transitions) > 0:\n",
    "    # Count transitions\n",
    "    trans_counts = transitions.group_by(['old_state_name', 'new_state_name']).len().sort('len', descending=True)\n",
    "    \n",
    "    # Create a simple flow diagram representation\n",
    "    states = set()\n",
    "    for row in trans_counts.iter_rows():\n",
    "        states.add(row[0])\n",
    "        states.add(row[1])\n",
    "    \n",
    "    # Plot top transitions as a bar chart\n",
    "    trans_labels = [f\"{row[0]} → {row[1]}\" for row in trans_counts.head(15).iter_rows()]\n",
    "    trans_values = [row[2] for row in trans_counts.head(15).iter_rows()]\n",
    "    \n",
    "    ax1.barh(range(len(trans_labels)), trans_values)\n",
    "    ax1.set_yticks(range(len(trans_labels)))\n",
    "    ax1.set_yticklabels(trans_labels)\n",
    "    ax1.set_xlabel('Count')\n",
    "    ax1.set_title('Top 15 State Transitions')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Event timeline\n",
    "# Create time-based event density\n",
    "start_time = tcp_state_df['ts_uptime_us'].min()\n",
    "tcp_state_df_time = tcp_state_df.with_columns([\n",
    "    ((pl.col('ts_uptime_us') - start_time) / 1_000_000).alias('time_seconds')\n",
    "])\n",
    "\n",
    "# Create histogram of events over time\n",
    "time_bins = np.linspace(0, tcp_state_df_time['time_seconds'].max(), 50)\n",
    "transitions_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'TRANSITION')['time_seconds'].to_numpy()\n",
    "processing_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'PROCESSING')['time_seconds'].to_numpy()\n",
    "errors_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'ERROR')['time_seconds'].to_numpy()\n",
    "\n",
    "ax2.hist([transitions_time, processing_time, errors_time], \n",
    "         bins=time_bins, label=['Transitions', 'Processing', 'Errors'], \n",
    "         stacked=True, alpha=0.7)\n",
    "ax2.set_xlabel('Time (seconds)')\n",
    "ax2.set_ylabel('Event Count')\n",
    "ax2.set_title('TCP State Events Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 6: Correlate with tcp_v4_rcv data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CORRELATION WITH TCP_V4_RCV BRANCH DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find overlapping time windows\n",
    "tcp_v4_time_range = (tcp_df['ts_uptime_us'].min(), tcp_df['ts_uptime_us'].max())\n",
    "tcp_state_time_range = (tcp_state_df['ts_uptime_us'].min(), tcp_state_df['ts_uptime_us'].max())\n",
    "\n",
    "overlap_start = max(tcp_v4_time_range[0], tcp_state_time_range[0])\n",
    "overlap_end = min(tcp_v4_time_range[1], tcp_state_time_range[1])\n",
    "\n",
    "if overlap_start < overlap_end:\n",
    "    print(f\"Time overlap: {(overlap_end - overlap_start) / 1_000_000:.2f} seconds\")\n",
    "    \n",
    "    # Filter both datasets to overlapping time\n",
    "    tcp_v4_overlap = tcp_df.filter(\n",
    "        (pl.col('ts_uptime_us') >= overlap_start) & \n",
    "        (pl.col('ts_uptime_us') <= overlap_end)\n",
    "    )\n",
    "    tcp_state_overlap = tcp_state_df.filter(\n",
    "        (pl.col('ts_uptime_us') >= overlap_start) & \n",
    "        (pl.col('ts_uptime_us') <= overlap_end)\n",
    "    )\n",
    "    \n",
    "    # Analyze patterns\n",
    "    print(f\"\\nDuring overlap period:\")\n",
    "    print(f\"- TCP v4 receive events: {len(tcp_v4_overlap):,}\")\n",
    "    print(f\"- TCP state events: {len(tcp_state_overlap):,}\")\n",
    "    \n",
    "    # Show branch distribution during state transitions\n",
    "    print(\"\\nTCP v4 branches during state processing:\")\n",
    "    branch_dist = tcp_v4_overlap.group_by('branch_name').len().sort('len', descending=True)\n",
    "    for row in branch_dist.head(10).iter_rows():\n",
    "        print(f\"  {row[0]:25}: {row[1]:6,} ({row[1]/len(tcp_v4_overlap)*100:5.1f}%)\")\n",
    "\n",
    "# Cell 7: Performance insights\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate metrics\n",
    "total_time = (tcp_state_df['ts_uptime_us'].max() - tcp_state_df['ts_uptime_us'].min()) / 1_000_000\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "errors = tcp_state_df.filter(pl.col('event_type_name') == 'ERROR')\n",
    "\n",
    "# Connection establishment rate\n",
    "new_connections = transitions.filter(pl.col('new_state_name') == 'ESTABLISHED')\n",
    "connection_rate = len(new_connections) / total_time if total_time > 0 else 0\n",
    "\n",
    "# Error rate\n",
    "error_rate = len(errors) / len(tcp_state_df) * 100 if len(tcp_state_df) > 0 else 0\n",
    "\n",
    "# State distribution\n",
    "state_dist = tcp_state_df.filter(pl.col('event_type_name') == 'PROCESSING').group_by('new_state_name').len()\n",
    "\n",
    "print(f\"Collection duration: {total_time:.2f} seconds\")\n",
    "print(f\"Connection establishment rate: {connection_rate:.1f} connections/second\")\n",
    "print(f\"Error rate: {error_rate:.2f}%\")\n",
    "print(f\"Total state transitions: {len(transitions):,}\")\n",
    "\n",
    "print(\"\\nState distribution (PROCESSING events):\")\n",
    "for row in state_dist.sort('len', descending=True).iter_rows():\n",
    "    print(f\"  {row[0]:15}: {row[1]:6,} events\")\n",
    "\n",
    "# Identify potential issues\n",
    "print(\"\\nPotential Issues:\")\n",
    "if error_rate > 1:\n",
    "    print(f\"⚠️  High error rate detected: {error_rate:.2f}%\")\n",
    "    \n",
    "challenge_acks = tcp_state_df.filter(pl.col('event_subtype_name') == 'CHALLENGE_ACK')\n",
    "if len(challenge_acks) > 0:\n",
    "    print(f\"⚠️  Challenge ACKs detected: {len(challenge_acks)} (possible SYN flood)\")\n",
    "    \n",
    "resets = tcp_state_df.filter(pl.col('event_subtype_name') == 'RESET')\n",
    "if len(resets) > 0:\n",
    "    print(f\"⚠️  Connection resets: {len(resets)}\")\n",
    "\n",
    "# Check for connection leaks\n",
    "time_wait_trans = transitions.filter(pl.col('new_state_name') == 'TIME_WAIT')\n",
    "close_trans = transitions.filter(pl.col('new_state_name').str.contains('CLOSE'))\n",
    "if len(new_connections) > 0:\n",
    "    close_ratio = (len(time_wait_trans) + len(close_trans)) / len(new_connections)\n",
    "    if close_ratio < 0.8:\n",
    "        print(f\"⚠️  Possible connection leak: only {close_ratio*100:.1f}% of connections properly closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841813f-c1fd-49d6-8092-89b08117e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple iperf3 Test - Docker Friendly Version\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# First, let's manually test if iperf3 works\n",
    "print(\"Testing iperf3 installation...\")\n",
    "\n",
    "# Install iperf3 if needed\n",
    "install_result = subprocess.run([\"which\", \"iperf3\"], capture_output=True)\n",
    "if install_result.returncode != 0:\n",
    "    print(\"Installing iperf3...\")\n",
    "    subprocess.run([\"apt-get\", \"update\"], capture_output=True)\n",
    "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"iperf3\"], capture_output=True)\n",
    "    print(\"iperf3 installed!\")\n",
    "else:\n",
    "    print(\"iperf3 is already installed\")\n",
    "\n",
    "# Kill any existing iperf3 processes\n",
    "subprocess.run([\"pkill\", \"-f\", \"iperf3\"], capture_output=True)\n",
    "time.sleep(1)\n",
    "\n",
    "# Start iperf3 server manually\n",
    "print(\"\\nStarting iperf3 server on port 5555...\")\n",
    "server = subprocess.Popen(\n",
    "    [\"iperf3\", \"-s\", \"-p\", \"5555\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "time.sleep(3)\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing server connection...\")\n",
    "test = subprocess.run(\n",
    "    [\"iperf3\", \"-c\", \"127.0.0.1\", \"-p\", \"5555\", \"-t\", \"1\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if test.returncode == 0:\n",
    "    print(\"✓ iperf3 is working!\")\n",
    "    \n",
    "    # Now run actual benchmark with kernmlops\n",
    "    print(\"\\nRunning benchmark with kernmlops...\")\n",
    "    \n",
    "    # Use minimal config for Docker\n",
    "    collect = Collector(\"./config/iperf_docker.yaml\")\n",
    "    \n",
    "    try:\n",
    "        collect.start_collection(None)\n",
    "        data = collect.wait()\n",
    "        \n",
    "        import polars as pl\n",
    "        tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "        print(f\"\\n✓ Success! Captured {len(tcp_df):,} TCP events\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Benchmark failed: {e}\")\n",
    "        print(\"\\nTry using the alternative method below...\")\n",
    "        \n",
    "else:\n",
    "    print(\"✗ iperf3 server test failed\")\n",
    "    print(f\"Error: {test.stderr}\")\n",
    "\n",
    "# Cleanup\n",
    "server.terminate()\n",
    "subprocess.run([\"pkill\", \"-f\", \"iperf3\"], capture_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ad3c7-0c6d-4da6-bde8-9d2bec08be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by process\n",
    "print(tcp_df.group_by(\"comm\").count().sort(\"count\", descending=True))\n",
    "iperf_client = tcp_df.filter(pl.col(\"comm\").str.contains(\"iperf3\").and_(~pl.col(\"comm\").str.contains(\"-s\")))\n",
    "\n",
    "print(f\"\\nProcess breakdown:\")\n",
    "print(f\"- iperf3 client: {len(iperf_client)} events\")\n",
    "\n",
    "# Analyze port 5555 traffic (iperf3 default port)\n",
    "port_5555 = tcp_df.filter((pl.col(\"dport\") == 5555) | (pl.col(\"sport\") == 5555))\n",
    "print(f\"- Port 5555 traffic: {len(port_5555)} events\")\n",
    "\n",
    "# Branch distribution\n",
    "print(\"\\nTCP state distribution:\")\n",
    "branch_dist = tcp_df.group_by(\"branch_name\").agg([\n",
    "    pl.count().alias(\"count\"),\n",
    "    (pl.count() / len(tcp_df) * 100).alias(\"percentage\")\n",
    "]).sort(\"count\", descending=True)\n",
    "\n",
    "for row in branch_dist.head(10).iter_rows():\n",
    "    print(f\"- {row[0]}: {row[1]:,} events ({row[2]:.1f}%)\")\n",
    "\n",
    "# Connection analysis\n",
    "new_connections = tcp_df.filter(pl.col(\"branch_name\") == \"new_syn_recv\")\n",
    "print(f\"\\nNew TCP connections: {len(new_connections)}\")\n",
    "print(f\"Connections per stream: ~{len(new_connections) / 4:.0f}\")  # 4 parallel streams\n",
    "\n",
    "# Drop analysis\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "if len(drops) > 0:\n",
    "    print(f\"\\nDropped packets: {len(drops)}\")\n",
    "    drop_dist = drops.group_by(\"drop_reason_name\").len()\n",
    "    for row in drop_dist.iter_rows():\n",
    "        print(f\"- {row[0]}: {row[1]} drops\")\n",
    "else:\n",
    "    print(\"\\nNo dropped packets detected - excellent!\")\n",
    "\n",
    "# Show branch distribution\n",
    "print(\"iperf_client group by branch_name\")\n",
    "print(iperf_client.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Show drop reasons\n",
    "drops = iperf_client.filter(pl.col(\"drop_reason\") > 0)\n",
    "print(drops.group_by(\"drop_reason_name\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dee3a5-43cc-4760-8024-3319c9ae9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_state_df = pl.read_parquet(data[\"tcp_state_process\"][0])\n",
    "\n",
    "print(f\"Loaded {len(tcp_df):,} tcp_v4_rcv events\")\n",
    "print(f\"Loaded {len(tcp_state_df):,} tcp_state_process events\")\n",
    "\n",
    "# Cell 2: Basic exploration of TCP state data\n",
    "# Show sample of state transitions\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "print(\"\\nSample State Transitions:\")\n",
    "print(transitions.select(['comm', 'old_state_name', 'new_state_name', 'ts_uptime_us']).head(10))\n",
    "\n",
    "# Show event type distribution\n",
    "print(\"\\nEvent Type Distribution:\")\n",
    "print(tcp_state_df.group_by('event_type_name').len().sort('len', descending=True))\n",
    "\n",
    "# Cell 3: Deep dive into specific process (e.g., nc or iperf3)\n",
    "# Filter for a specific process\n",
    "process_name = \"nc\"  # or \"iperf3\" depending on your test\n",
    "process_events = tcp_state_df.filter(pl.col('comm').str.contains(process_name))\n",
    "\n",
    "if len(process_events) > 0:\n",
    "    print(f\"\\n{process_name} TCP State Analysis:\")\n",
    "    print(f\"Total events: {len(process_events)}\")\n",
    "    \n",
    "    # Show state progression\n",
    "    print(\"\\nState progression timeline:\")\n",
    "    timeline = process_events.select(['ts_uptime_us', 'event_type_name', 'old_state_name', 'new_state_name']).sort('ts_uptime_us')\n",
    "    \n",
    "    # Convert to relative time in milliseconds\n",
    "    start_time = timeline['ts_uptime_us'].min()\n",
    "    timeline_with_relative = timeline.with_columns([\n",
    "        ((pl.col('ts_uptime_us') - start_time) / 1000).alias('relative_ms')\n",
    "    ])\n",
    "    \n",
    "    # Show key events\n",
    "    for row in timeline_with_relative.head(20).iter_rows():\n",
    "        if row[1] == 'TRANSITION':\n",
    "            print(f\"{row[4]:8.1f}ms: {row[2]} → {row[3]}\")\n",
    "        else:\n",
    "            print(f\"{row[4]:8.1f}ms: {row[1]} in {row[3]}\")\n",
    "\n",
    "# Cell 4: Connection lifecycle analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONNECTION LIFECYCLE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Group events by process and analyze connection patterns\n",
    "connection_patterns = tcp_state_df.group_by('comm').agg([\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'SYN_SENT').len().alias('syn_sent_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'SYN_RECV').len().alias('syn_recv_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'ESTABLISHED').len().alias('established_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'TIME_WAIT').len().alias('time_wait_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('event_type_name') == 'ERROR').len().alias('error_count')\n",
    "]).filter(pl.col('established_count') > 0).sort('established_count', descending=True)\n",
    "\n",
    "print(\"Connection patterns by process:\")\n",
    "print(connection_patterns.head(10))\n",
    "\n",
    "# Cell 5: Visualize state machine flow\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# State transition graph\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "if len(transitions) > 0:\n",
    "    # Count transitions\n",
    "    trans_counts = transitions.group_by(['old_state_name', 'new_state_name']).len().sort('len', descending=True)\n",
    "    \n",
    "    # Create a simple flow diagram representation\n",
    "    states = set()\n",
    "    for row in trans_counts.iter_rows():\n",
    "        states.add(row[0])\n",
    "        states.add(row[1])\n",
    "    \n",
    "    # Plot top transitions as a bar chart\n",
    "    trans_labels = [f\"{row[0]} → {row[1]}\" for row in trans_counts.head(15).iter_rows()]\n",
    "    trans_values = [row[2] for row in trans_counts.head(15).iter_rows()]\n",
    "    \n",
    "    ax1.barh(range(len(trans_labels)), trans_values)\n",
    "    ax1.set_yticks(range(len(trans_labels)))\n",
    "    ax1.set_yticklabels(trans_labels)\n",
    "    ax1.set_xlabel('Count')\n",
    "    ax1.set_title('Top 15 State Transitions')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Event timeline\n",
    "# Create time-based event density\n",
    "start_time = tcp_state_df['ts_uptime_us'].min()\n",
    "tcp_state_df_time = tcp_state_df.with_columns([\n",
    "    ((pl.col('ts_uptime_us') - start_time) / 1_000_000).alias('time_seconds')\n",
    "])\n",
    "\n",
    "# Create histogram of events over time\n",
    "time_bins = np.linspace(0, tcp_state_df_time['time_seconds'].max(), 50)\n",
    "transitions_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'TRANSITION')['time_seconds'].to_numpy()\n",
    "processing_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'PROCESSING')['time_seconds'].to_numpy()\n",
    "errors_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'ERROR')['time_seconds'].to_numpy()\n",
    "\n",
    "ax2.hist([transitions_time, processing_time, errors_time], \n",
    "         bins=time_bins, label=['Transitions', 'Processing', 'Errors'], \n",
    "         stacked=True, alpha=0.7)\n",
    "ax2.set_xlabel('Time (seconds)')\n",
    "ax2.set_ylabel('Event Count')\n",
    "ax2.set_title('TCP State Events Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 6: Correlate with tcp_v4_rcv data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CORRELATION WITH TCP_V4_RCV BRANCH DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find overlapping time windows\n",
    "tcp_v4_time_range = (tcp_df['ts_uptime_us'].min(), tcp_df['ts_uptime_us'].max())\n",
    "tcp_state_time_range = (tcp_state_df['ts_uptime_us'].min(), tcp_state_df['ts_uptime_us'].max())\n",
    "\n",
    "overlap_start = max(tcp_v4_time_range[0], tcp_state_time_range[0])\n",
    "overlap_end = min(tcp_v4_time_range[1], tcp_state_time_range[1])\n",
    "\n",
    "if overlap_start < overlap_end:\n",
    "    print(f\"Time overlap: {(overlap_end - overlap_start) / 1_000_000:.2f} seconds\")\n",
    "    \n",
    "    # Filter both datasets to overlapping time\n",
    "    tcp_v4_overlap = tcp_df.filter(\n",
    "        (pl.col('ts_uptime_us') >= overlap_start) & \n",
    "        (pl.col('ts_uptime_us') <= overlap_end)\n",
    "    )\n",
    "    tcp_state_overlap = tcp_state_df.filter(\n",
    "        (pl.col('ts_uptime_us') >= overlap_start) & \n",
    "        (pl.col('ts_uptime_us') <= overlap_end)\n",
    "    )\n",
    "    \n",
    "    # Analyze patterns\n",
    "    print(f\"\\nDuring overlap period:\")\n",
    "    print(f\"- TCP v4 receive events: {len(tcp_v4_overlap):,}\")\n",
    "    print(f\"- TCP state events: {len(tcp_state_overlap):,}\")\n",
    "    \n",
    "    # Show branch distribution during state transitions\n",
    "    print(\"\\nTCP v4 branches during state processing:\")\n",
    "    branch_dist = tcp_v4_overlap.group_by('branch_name').len().sort('len', descending=True)\n",
    "    for row in branch_dist.head(10).iter_rows():\n",
    "        print(f\"  {row[0]:25}: {row[1]:6,} ({row[1]/len(tcp_v4_overlap)*100:5.1f}%)\")\n",
    "\n",
    "# Cell 7: Performance insights\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate metrics\n",
    "total_time = (tcp_state_df['ts_uptime_us'].max() - tcp_state_df['ts_uptime_us'].min()) / 1_000_000\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "errors = tcp_state_df.filter(pl.col('event_type_name') == 'ERROR')\n",
    "\n",
    "# Connection establishment rate\n",
    "new_connections = transitions.filter(pl.col('new_state_name') == 'ESTABLISHED')\n",
    "connection_rate = len(new_connections) / total_time if total_time > 0 else 0\n",
    "\n",
    "# Error rate\n",
    "error_rate = len(errors) / len(tcp_state_df) * 100 if len(tcp_state_df) > 0 else 0\n",
    "\n",
    "# State distribution\n",
    "state_dist = tcp_state_df.filter(pl.col('event_type_name') == 'PROCESSING').group_by('new_state_name').len()\n",
    "\n",
    "print(f\"Collection duration: {total_time:.2f} seconds\")\n",
    "print(f\"Connection establishment rate: {connection_rate:.1f} connections/second\")\n",
    "print(f\"Error rate: {error_rate:.2f}%\")\n",
    "print(f\"Total state transitions: {len(transitions):,}\")\n",
    "\n",
    "print(\"\\nState distribution (PROCESSING events):\")\n",
    "for row in state_dist.sort('len', descending=True).iter_rows():\n",
    "    print(f\"  {row[0]:15}: {row[1]:6,} events\")\n",
    "\n",
    "# Identify potential issues\n",
    "print(\"\\nPotential Issues:\")\n",
    "if error_rate > 1:\n",
    "    print(f\"⚠️  High error rate detected: {error_rate:.2f}%\")\n",
    "    \n",
    "challenge_acks = tcp_state_df.filter(pl.col('event_subtype_name') == 'CHALLENGE_ACK')\n",
    "if len(challenge_acks) > 0:\n",
    "    print(f\"⚠️  Challenge ACKs detected: {len(challenge_acks)} (possible SYN flood)\")\n",
    "    \n",
    "resets = tcp_state_df.filter(pl.col('event_subtype_name') == 'RESET')\n",
    "if len(resets) > 0:\n",
    "    print(f\"⚠️  Connection resets: {len(resets)}\")\n",
    "\n",
    "# Check for connection leaks\n",
    "time_wait_trans = transitions.filter(pl.col('new_state_name') == 'TIME_WAIT')\n",
    "close_trans = transitions.filter(pl.col('new_state_name').str.contains('CLOSE'))\n",
    "if len(new_connections) > 0:\n",
    "    close_ratio = (len(time_wait_trans) + len(close_trans)) / len(new_connections)\n",
    "    if close_ratio < 0.8:\n",
    "        print(f\"⚠️  Possible connection leak: only {close_ratio*100:.1f}% of connections properly closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e59363-0273-4ed1-bfd3-cabda586c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = Collector(\"./config/redis_never.yaml\")\n",
    "collect.start_collection(None)\n",
    "data = collect.stop_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8fc10-9a43-4751-9581-eb33f1b61a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TCP branches\n",
    "import polars as pl\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "print(tcp_df.group_by(\"comm\").count().sort(\"count\", descending=True))\n",
    "\n",
    "redis = tcp_df.filter(pl.col(\"comm\").str.contains(\"redis-server\"))\n",
    "\n",
    "# Show branch distribution\n",
    "print(redis.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Show drop reasons\n",
    "drops = redis.filter(pl.col(\"drop_reason\") > 0)\n",
    "print(drops.group_by(\"drop_reason_name\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121ed65-4d89-44e6-b66c-1a3f599390c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_state_df = pl.read_parquet(data[\"tcp_state_process\"][0])\n",
    "\n",
    "print(f\"Loaded {len(tcp_df):,} tcp_v4_rcv events\")\n",
    "print(f\"Loaded {len(tcp_state_df):,} tcp_state_process events\")\n",
    "\n",
    "# Cell 2: Basic exploration of TCP state data\n",
    "# Show sample of state transitions\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "print(\"\\nSample State Transitions:\")\n",
    "print(transitions.select(['comm', 'old_state_name', 'new_state_name', 'ts_uptime_us']).head(10))\n",
    "\n",
    "# Show event type distribution\n",
    "print(\"\\nEvent Type Distribution:\")\n",
    "print(tcp_state_df.group_by('event_type_name').len().sort('len', descending=True))\n",
    "\n",
    "# Cell 3: Deep dive into specific process (e.g., nc or iperf3)\n",
    "# Filter for a specific process\n",
    "process_name = \"nc\"  # or \"iperf3\" depending on your test\n",
    "process_events = tcp_state_df.filter(pl.col('comm').str.contains(process_name))\n",
    "\n",
    "if len(process_events) > 0:\n",
    "    print(f\"\\n{process_name} TCP State Analysis:\")\n",
    "    print(f\"Total events: {len(process_events)}\")\n",
    "    \n",
    "    # Show state progression\n",
    "    print(\"\\nState progression timeline:\")\n",
    "    timeline = process_events.select(['ts_uptime_us', 'event_type_name', 'old_state_name', 'new_state_name']).sort('ts_uptime_us')\n",
    "    \n",
    "    # Convert to relative time in milliseconds\n",
    "    start_time = timeline['ts_uptime_us'].min()\n",
    "    timeline_with_relative = timeline.with_columns([\n",
    "        ((pl.col('ts_uptime_us') - start_time) / 1000).alias('relative_ms')\n",
    "    ])\n",
    "    \n",
    "    # Show key events\n",
    "    for row in timeline_with_relative.head(20).iter_rows():\n",
    "        if row[1] == 'TRANSITION':\n",
    "            print(f\"{row[4]:8.1f}ms: {row[2]} → {row[3]}\")\n",
    "        else:\n",
    "            print(f\"{row[4]:8.1f}ms: {row[1]} in {row[3]}\")\n",
    "\n",
    "# Cell 4: Connection lifecycle analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONNECTION LIFECYCLE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Group events by process and analyze connection patterns\n",
    "connection_patterns = tcp_state_df.group_by('comm').agg([\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'SYN_SENT').len().alias('syn_sent_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'SYN_RECV').len().alias('syn_recv_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'ESTABLISHED').len().alias('established_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'TIME_WAIT').len().alias('time_wait_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('event_type_name') == 'ERROR').len().alias('error_count')\n",
    "]).filter(pl.col('established_count') > 0).sort('established_count', descending=True)\n",
    "\n",
    "print(\"Connection patterns by process:\")\n",
    "print(connection_patterns.head(10))\n",
    "\n",
    "# Cell 5: Visualize state machine flow\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# State transition graph\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "if len(transitions) > 0:\n",
    "    # Count transitions\n",
    "    trans_counts = transitions.group_by(['old_state_name', 'new_state_name']).len().sort('len', descending=True)\n",
    "    \n",
    "    # Create a simple flow diagram representation\n",
    "    states = set()\n",
    "    for row in trans_counts.iter_rows():\n",
    "        states.add(row[0])\n",
    "        states.add(row[1])\n",
    "    \n",
    "    # Plot top transitions as a bar chart\n",
    "    trans_labels = [f\"{row[0]} → {row[1]}\" for row in trans_counts.head(15).iter_rows()]\n",
    "    trans_values = [row[2] for row in trans_counts.head(15).iter_rows()]\n",
    "    \n",
    "    ax1.barh(range(len(trans_labels)), trans_values)\n",
    "    ax1.set_yticks(range(len(trans_labels)))\n",
    "    ax1.set_yticklabels(trans_labels)\n",
    "    ax1.set_xlabel('Count')\n",
    "    ax1.set_title('Top 15 State Transitions')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Event timeline\n",
    "# Create time-based event density\n",
    "start_time = tcp_state_df['ts_uptime_us'].min()\n",
    "tcp_state_df_time = tcp_state_df.with_columns([\n",
    "    ((pl.col('ts_uptime_us') - start_time) / 1_000_000).alias('time_seconds')\n",
    "])\n",
    "\n",
    "# Create histogram of events over time\n",
    "time_bins = np.linspace(0, tcp_state_df_time['time_seconds'].max(), 50)\n",
    "transitions_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'TRANSITION')['time_seconds'].to_numpy()\n",
    "processing_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'PROCESSING')['time_seconds'].to_numpy()\n",
    "errors_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'ERROR')['time_seconds'].to_numpy()\n",
    "\n",
    "ax2.hist([transitions_time, processing_time, errors_time], \n",
    "         bins=time_bins, label=['Transitions', 'Processing', 'Errors'], \n",
    "         stacked=True, alpha=0.7)\n",
    "ax2.set_xlabel('Time (seconds)')\n",
    "ax2.set_ylabel('Event Count')\n",
    "ax2.set_title('TCP State Events Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 6: Correlate with tcp_v4_rcv data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CORRELATION WITH TCP_V4_RCV BRANCH DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find overlapping time windows\n",
    "tcp_v4_time_range = (tcp_df['ts_uptime_us'].min(), tcp_df['ts_uptime_us'].max())\n",
    "tcp_state_time_range = (tcp_state_df['ts_uptime_us'].min(), tcp_state_df['ts_uptime_us'].max())\n",
    "\n",
    "overlap_start = max(tcp_v4_time_range[0], tcp_state_time_range[0])\n",
    "overlap_end = min(tcp_v4_time_range[1], tcp_state_time_range[1])\n",
    "\n",
    "if overlap_start < overlap_end:\n",
    "    print(f\"Time overlap: {(overlap_end - overlap_start) / 1_000_000:.2f} seconds\")\n",
    "    \n",
    "    # Filter both datasets to overlapping time\n",
    "    tcp_v4_overlap = tcp_df.filter(\n",
    "        (pl.col('ts_uptime_us') >= overlap_start) & \n",
    "        (pl.col('ts_uptime_us') <= overlap_end)\n",
    "    )\n",
    "    tcp_state_overlap = tcp_state_df.filter(\n",
    "        (pl.col('ts_uptime_us') >= overlap_start) & \n",
    "        (pl.col('ts_uptime_us') <= overlap_end)\n",
    "    )\n",
    "    \n",
    "    # Analyze patterns\n",
    "    print(f\"\\nDuring overlap period:\")\n",
    "    print(f\"- TCP v4 receive events: {len(tcp_v4_overlap):,}\")\n",
    "    print(f\"- TCP state events: {len(tcp_state_overlap):,}\")\n",
    "    \n",
    "    # Show branch distribution during state transitions\n",
    "    print(\"\\nTCP v4 branches during state processing:\")\n",
    "    branch_dist = tcp_v4_overlap.group_by('branch_name').len().sort('len', descending=True)\n",
    "    for row in branch_dist.head(10).iter_rows():\n",
    "        print(f\"  {row[0]:25}: {row[1]:6,} ({row[1]/len(tcp_v4_overlap)*100:5.1f}%)\")\n",
    "\n",
    "# Cell 7: Performance insights\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate metrics\n",
    "total_time = (tcp_state_df['ts_uptime_us'].max() - tcp_state_df['ts_uptime_us'].min()) / 1_000_000\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "errors = tcp_state_df.filter(pl.col('event_type_name') == 'ERROR')\n",
    "\n",
    "# Connection establishment rate\n",
    "new_connections = transitions.filter(pl.col('new_state_name') == 'ESTABLISHED')\n",
    "connection_rate = len(new_connections) / total_time if total_time > 0 else 0\n",
    "\n",
    "# Error rate\n",
    "error_rate = len(errors) / len(tcp_state_df) * 100 if len(tcp_state_df) > 0 else 0\n",
    "\n",
    "# State distribution\n",
    "state_dist = tcp_state_df.filter(pl.col('event_type_name') == 'PROCESSING').group_by('new_state_name').len()\n",
    "\n",
    "print(f\"Collection duration: {total_time:.2f} seconds\")\n",
    "print(f\"Connection establishment rate: {connection_rate:.1f} connections/second\")\n",
    "print(f\"Error rate: {error_rate:.2f}%\")\n",
    "print(f\"Total state transitions: {len(transitions):,}\")\n",
    "\n",
    "print(\"\\nState distribution (PROCESSING events):\")\n",
    "for row in state_dist.sort('len', descending=True).iter_rows():\n",
    "    print(f\"  {row[0]:15}: {row[1]:6,} events\")\n",
    "\n",
    "# Identify potential issues\n",
    "print(\"\\nPotential Issues:\")\n",
    "if error_rate > 1:\n",
    "    print(f\"⚠️  High error rate detected: {error_rate:.2f}%\")\n",
    "    \n",
    "challenge_acks = tcp_state_df.filter(pl.col('event_subtype_name') == 'CHALLENGE_ACK')\n",
    "if len(challenge_acks) > 0:\n",
    "    print(f\"⚠️  Challenge ACKs detected: {len(challenge_acks)} (possible SYN flood)\")\n",
    "    \n",
    "resets = tcp_state_df.filter(pl.col('event_subtype_name') == 'RESET')\n",
    "if len(resets) > 0:\n",
    "    print(f\"⚠️  Connection resets: {len(resets)}\")\n",
    "\n",
    "# Check for connection leaks\n",
    "time_wait_trans = transitions.filter(pl.col('new_state_name') == 'TIME_WAIT')\n",
    "close_trans = transitions.filter(pl.col('new_state_name').str.contains('CLOSE'))\n",
    "if len(new_connections) > 0:\n",
    "    close_ratio = (len(time_wait_trans) + len(close_trans)) / len(new_connections)\n",
    "    if close_ratio < 0.8:\n",
    "        print(f\"⚠️  Possible connection leak: only {close_ratio*100:.1f}% of connections properly closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1994657-5088-4a55-8c97-785073912778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collector with XSBench configuration\n",
    "collect = Collector(\"./config/xsbench.yaml\")\n",
    "\n",
    "# Start collection and run XSBench\n",
    "print(\"Starting collection with XSBench workload...\")\n",
    "collect.start_collection(None)\n",
    "\n",
    "# Wait for XSBench to complete\n",
    "print(\"Running XSBench benchmark...\")\n",
    "data = collect.stop_collection()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad914c-bd29-4e80-a5c9-f37cb88f14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TCP traffic generated by XSBench\n",
    "print(\"\\nAnalyzing TCP traffic from XSBench:\")\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "# Show branch distribution\n",
    "print(\"\\nTCP branch distribution:\")\n",
    "print(tcp_df.group_by(\"branch_name\").len().sort(\"len\", descending=True))\n",
    "\n",
    "# Show drop reasons if any\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "if len(drops) > 0:\n",
    "    print(\"\\nDropped packets:\")\n",
    "    print(drops.group_by(\"drop_reason_name\").len())\n",
    "else:\n",
    "    print(\"\\nNo dropped packets detected\")\n",
    "\n",
    "# Show process-specific TCP activity\n",
    "print(\"\\nTCP activity by process:\")\n",
    "process_tcp = tcp_df.group_by(\"comm\").len().sort(\"len\", descending=True).head(10)\n",
    "print(process_tcp)\n",
    "\n",
    "# Check for XSBench-specific activity\n",
    "xsbench_traffic = tcp_df.filter(pl.col(\"comm\").str.contains(\"XSBench\"))\n",
    "if len(xsbench_traffic) > 0:\n",
    "    print(f\"\\nXSBench generated {len(xsbench_traffic)} TCP events\")\n",
    "else:\n",
    "    print(\"\\nNo direct TCP traffic from XSBench process detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586429f8-e5e7-41b8-b62a-5d5779074ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_state_df = pl.read_parquet(data[\"tcp_state_process\"][0])\n",
    "\n",
    "print(f\"Loaded {len(tcp_df):,} tcp_v4_rcv events\")\n",
    "print(f\"Loaded {len(tcp_state_df):,} tcp_state_process events\")\n",
    "\n",
    "# Cell 2: Basic exploration of TCP state data\n",
    "# Show sample of state transitions\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "print(\"\\nSample State Transitions:\")\n",
    "print(transitions.select(['comm', 'old_state_name', 'new_state_name', 'ts_uptime_us']).head(10))\n",
    "\n",
    "# Show event type distribution\n",
    "print(\"\\nEvent Type Distribution:\")\n",
    "print(tcp_state_df.group_by('event_type_name').len().sort('len', descending=True))\n",
    "\n",
    "# Cell 3: Deep dive into specific process (e.g., nc or iperf3)\n",
    "# Filter for a specific process\n",
    "process_name = \"nc\"  # or \"iperf3\" depending on your test\n",
    "process_events = tcp_state_df.filter(pl.col('comm').str.contains(process_name))\n",
    "\n",
    "if len(process_events) > 0:\n",
    "    print(f\"\\n{process_name} TCP State Analysis:\")\n",
    "    print(f\"Total events: {len(process_events)}\")\n",
    "    \n",
    "    # Show state progression\n",
    "    print(\"\\nState progression timeline:\")\n",
    "    timeline = process_events.select(['ts_uptime_us', 'event_type_name', 'old_state_name', 'new_state_name']).sort('ts_uptime_us')\n",
    "    \n",
    "    # Convert to relative time in milliseconds\n",
    "    start_time = timeline['ts_uptime_us'].min()\n",
    "    timeline_with_relative = timeline.with_columns([\n",
    "        ((pl.col('ts_uptime_us') - start_time) / 1000).alias('relative_ms')\n",
    "    ])\n",
    "    \n",
    "    # Show key events\n",
    "    for row in timeline_with_relative.head(20).iter_rows():\n",
    "        if row[1] == 'TRANSITION':\n",
    "            print(f\"{row[4]:8.1f}ms: {row[2]} → {row[3]}\")\n",
    "        else:\n",
    "            print(f\"{row[4]:8.1f}ms: {row[1]} in {row[3]}\")\n",
    "\n",
    "# Cell 4: Connection lifecycle analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONNECTION LIFECYCLE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Group events by process and analyze connection patterns\n",
    "connection_patterns = tcp_state_df.group_by('comm').agg([\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'SYN_SENT').len().alias('syn_sent_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'SYN_RECV').len().alias('syn_recv_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'ESTABLISHED').len().alias('established_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('new_state_name') == 'TIME_WAIT').len().alias('time_wait_count'),\n",
    "    pl.col('event_type_name').filter(pl.col('event_type_name') == 'ERROR').len().alias('error_count')\n",
    "]).filter(pl.col('established_count') > 0).sort('established_count', descending=True)\n",
    "\n",
    "print(\"Connection patterns by process:\")\n",
    "print(connection_patterns.head(10))\n",
    "\n",
    "# Cell 5: Visualize state machine flow\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# State transition graph\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "if len(transitions) > 0:\n",
    "    # Count transitions\n",
    "    trans_counts = transitions.group_by(['old_state_name', 'new_state_name']).len().sort('len', descending=True)\n",
    "    \n",
    "    # Create a simple flow diagram representation\n",
    "    states = set()\n",
    "    for row in trans_counts.iter_rows():\n",
    "        states.add(row[0])\n",
    "        states.add(row[1])\n",
    "    \n",
    "    # Plot top transitions as a bar chart\n",
    "    trans_labels = [f\"{row[0]} → {row[1]}\" for row in trans_counts.head(15).iter_rows()]\n",
    "    trans_values = [row[2] for row in trans_counts.head(15).iter_rows()]\n",
    "    \n",
    "    ax1.barh(range(len(trans_labels)), trans_values)\n",
    "    ax1.set_yticks(range(len(trans_labels)))\n",
    "    ax1.set_yticklabels(trans_labels)\n",
    "    ax1.set_xlabel('Count')\n",
    "    ax1.set_title('Top 15 State Transitions')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Event timeline\n",
    "# Create time-based event density\n",
    "start_time = tcp_state_df['ts_uptime_us'].min()\n",
    "tcp_state_df_time = tcp_state_df.with_columns([\n",
    "    ((pl.col('ts_uptime_us') - start_time) / 1_000_000).alias('time_seconds')\n",
    "])\n",
    "\n",
    "# Create histogram of events over time\n",
    "time_bins = np.linspace(0, tcp_state_df_time['time_seconds'].max(), 50)\n",
    "transitions_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'TRANSITION')['time_seconds'].to_numpy()\n",
    "processing_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'PROCESSING')['time_seconds'].to_numpy()\n",
    "errors_time = tcp_state_df_time.filter(pl.col('event_type_name') == 'ERROR')['time_seconds'].to_numpy()\n",
    "\n",
    "ax2.hist([transitions_time, processing_time, errors_time], \n",
    "         bins=time_bins, label=['Transitions', 'Processing', 'Errors'], \n",
    "         stacked=True, alpha=0.7)\n",
    "ax2.set_xlabel('Time (seconds)')\n",
    "ax2.set_ylabel('Event Count')\n",
    "ax2.set_title('TCP State Events Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 6: Correlate with tcp_v4_rcv data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CORRELATION WITH TCP_V4_RCV BRANCH DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find overlapping time windows\n",
    "tcp_v4_time_range = (tcp_df['ts_uptime_us'].min(), tcp_df['ts_uptime_us'].max())\n",
    "tcp_state_time_range = (tcp_state_df['ts_uptime_us'].min(), tcp_state_df['ts_uptime_us'].max())\n",
    "\n",
    "overlap_start = max(tcp_v4_time_range[0], tcp_state_time_range[0])\n",
    "overlap_end = min(tcp_v4_time_range[1], tcp_state_time_range[1])\n",
    "\n",
    "if overlap_start < overlap_end:\n",
    "    print(f\"Time overlap: {(overlap_end - overlap_start) / 1_000_000:.2f} seconds\")\n",
    "    \n",
    "    # Filter both datasets to overlapping time\n",
    "    tcp_v4_overlap = tcp_df.filter(\n",
    "        (pl.col('ts_uptime_us') >= overlap_start) & \n",
    "        (pl.col('ts_uptime_us') <= overlap_end)\n",
    "    )\n",
    "    tcp_state_overlap = tcp_state_df.filter(\n",
    "        (pl.col('ts_uptime_us') >= overlap_start) & \n",
    "        (pl.col('ts_uptime_us') <= overlap_end)\n",
    "    )\n",
    "    \n",
    "    # Analyze patterns\n",
    "    print(f\"\\nDuring overlap period:\")\n",
    "    print(f\"- TCP v4 receive events: {len(tcp_v4_overlap):,}\")\n",
    "    print(f\"- TCP state events: {len(tcp_state_overlap):,}\")\n",
    "    \n",
    "    # Show branch distribution during state transitions\n",
    "    print(\"\\nTCP v4 branches during state processing:\")\n",
    "    branch_dist = tcp_v4_overlap.group_by('branch_name').len().sort('len', descending=True)\n",
    "    for row in branch_dist.head(10).iter_rows():\n",
    "        print(f\"  {row[0]:25}: {row[1]:6,} ({row[1]/len(tcp_v4_overlap)*100:5.1f}%)\")\n",
    "\n",
    "# Cell 7: Performance insights\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate metrics\n",
    "total_time = (tcp_state_df['ts_uptime_us'].max() - tcp_state_df['ts_uptime_us'].min()) / 1_000_000\n",
    "transitions = tcp_state_df.filter(pl.col('event_type_name') == 'TRANSITION')\n",
    "errors = tcp_state_df.filter(pl.col('event_type_name') == 'ERROR')\n",
    "\n",
    "# Connection establishment rate\n",
    "new_connections = transitions.filter(pl.col('new_state_name') == 'ESTABLISHED')\n",
    "connection_rate = len(new_connections) / total_time if total_time > 0 else 0\n",
    "\n",
    "# Error rate\n",
    "error_rate = len(errors) / len(tcp_state_df) * 100 if len(tcp_state_df) > 0 else 0\n",
    "\n",
    "# State distribution\n",
    "state_dist = tcp_state_df.filter(pl.col('event_type_name') == 'PROCESSING').group_by('new_state_name').len()\n",
    "\n",
    "print(f\"Collection duration: {total_time:.2f} seconds\")\n",
    "print(f\"Connection establishment rate: {connection_rate:.1f} connections/second\")\n",
    "print(f\"Error rate: {error_rate:.2f}%\")\n",
    "print(f\"Total state transitions: {len(transitions):,}\")\n",
    "\n",
    "print(\"\\nState distribution (PROCESSING events):\")\n",
    "for row in state_dist.sort('len', descending=True).iter_rows():\n",
    "    print(f\"  {row[0]:15}: {row[1]:6,} events\")\n",
    "\n",
    "# Identify potential issues\n",
    "print(\"\\nPotential Issues:\")\n",
    "if error_rate > 1:\n",
    "    print(f\"⚠️  High error rate detected: {error_rate:.2f}%\")\n",
    "    \n",
    "challenge_acks = tcp_state_df.filter(pl.col('event_subtype_name') == 'CHALLENGE_ACK')\n",
    "if len(challenge_acks) > 0:\n",
    "    print(f\"⚠️  Challenge ACKs detected: {len(challenge_acks)} (possible SYN flood)\")\n",
    "    \n",
    "resets = tcp_state_df.filter(pl.col('event_subtype_name') == 'RESET')\n",
    "if len(resets) > 0:\n",
    "    print(f\"⚠️  Connection resets: {len(resets)}\")\n",
    "\n",
    "# Check for connection leaks\n",
    "time_wait_trans = transitions.filter(pl.col('new_state_name') == 'TIME_WAIT')\n",
    "close_trans = transitions.filter(pl.col('new_state_name').str.contains('CLOSE'))\n",
    "if len(new_connections) > 0:\n",
    "    close_ratio = (len(time_wait_trans) + len(close_trans)) / len(new_connections)\n",
    "    if close_ratio < 0.8:\n",
    "        print(f\"⚠️  Possible connection leak: only {close_ratio*100:.1f}% of connections properly closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
