{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da464b6-89fc-4ca2-9290-f3bebbe2dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make docker-image > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62bef45-339e-44c1-8309-874c236ed23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CONTAINER_CMD=\"bash -lc 'make install-ycsb\" make docker > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a1beeb-3882-4dd0-91c7-aa98c303c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pexpect\n",
    "import os\n",
    "import time\n",
    "\n",
    "\"\"\" Collector class has helper methods to interact with kermit\"\"\"\n",
    "class Collector: \n",
    "    def __init__(self, config: Path):\n",
    "        self.env = os.environ.copy()\n",
    "        self.env[\"INTERACTIVE\"] = \"it\"\n",
    "        self.env[\"CONTAINER_CMD\"] = f\"bash -lc 'KERNMLOPS_CONFIG_FILE={config} make collect-data'\"\n",
    "        self.collect_process : pexpect.spawn | None = None\n",
    "\n",
    "    def start_collection(self, logfile=None):\n",
    "        self.collect_process = pexpect.spawn(\"make docker\", env=self.env, timeout=None, logfile=logfile)\n",
    "        self.collect_process.expect_exact([\"Started benchmark\"])\n",
    "\n",
    "    def _after_run_generate_file_data() -> dict[str, list[Path]]:\n",
    "        start_path : Path = Path(\"./data\")\n",
    "        list_of_collect_id_dirs = start_path.glob(\"*/*/*\")\n",
    "        latest_collect_id = max(list_of_collect_id_dirs, key=os.path.getctime)\n",
    "        list_of_files = latest_collect_id.glob(\"*.*.parquet\")\n",
    "        output = {}\n",
    "        for f in list_of_files:\n",
    "            index = str(f).removeprefix(str(f.parent) + \"/\").split(\".\")[0]\n",
    "            if index not in output.keys():\n",
    "                output[index] = []\n",
    "            output[index].append(f)\n",
    "        return output\n",
    "        \n",
    "    def wait(self) -> int:\n",
    "        if self.collect_process is None:\n",
    "            return\n",
    "        self.collect_process.expect([pexpect.EOF])\n",
    "        self.collect_process.wait()\n",
    "        return Collector._after_run_generate_file_data()\n",
    "        \n",
    "    def stop_collection(self):\n",
    "        if self.collect_process is None:\n",
    "            return\n",
    "        self.collect_process.sendline(\"END\")\n",
    "        return self.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5874f9a-d168-446b-9a52-ec5b49f7c35f",
   "metadata": {},
   "source": [
    "There are two ways to run kermit:\n",
    "- With the raw config\n",
    "- With a pre-programmed benchmark config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4841813f-c1fd-49d6-8092-89b08117e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iperf3 TCP benchmark...\n",
      "This will generate high-volume TCP traffic for 30 seconds\n"
     ]
    },
    {
     "ename": "EOF",
     "evalue": "End Of File (EOF). Exception style platform.\n<pexpect.pty_spawn.spawn object at 0x7291e548c800>\ncommand: /usr/bin/make\nargs: ['/usr/bin/make', 'docker']\nbuffer (last 100 chars): b''\nbefore (last 100 chars): b'ors.BenchmarkError: iperf3 server not responding\\r\\n\\r\\nmake: *** [Makefile:107: collect-data] Error 1\\r\\n'\nafter: <class 'pexpect.exceptions.EOF'>\nmatch: None\nmatch_index: None\nexitstatus: None\nflag_eof: True\npid: 50962\nchild_fd: 58\nclosed: False\ntimeout: None\ndelimiter: <class 'pexpect.exceptions.EOF'>\nlogfile: None\nlogfile_read: None\nlogfile_send: None\nmaxread: 2000\nignorecase: False\nsearchwindowsize: None\ndelaybeforesend: 0.05\ndelayafterclose: 0.1\ndelayafterterminate: 0.1\nsearcher: searcher_string:\n    0: b'Started benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEOF\u001b[39m                                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting iperf3 TCP benchmark...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis will generate high-volume TCP traffic for 30 seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mcollect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Wait for benchmark to complete\u001b[39;00m\n\u001b[32m     14\u001b[39m data = collect.wait()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mCollector.start_collection\u001b[39m\u001b[34m(self, logfile)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart_collection\u001b[39m(\u001b[38;5;28mself\u001b[39m, logfile=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m.collect_process = pexpect.spawn(\u001b[33m\"\u001b[39m\u001b[33mmake docker\u001b[39m\u001b[33m\"\u001b[39m, env=\u001b[38;5;28mself\u001b[39m.env, timeout=\u001b[38;5;28;01mNone\u001b[39;00m, logfile=logfile)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_process\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpect_exact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStarted benchmark\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kernmlops/.venv/lib/python3.12/site-packages/pexpect/spawnbase.py:432\u001b[39m, in \u001b[36mSpawnBase.expect_exact\u001b[39m\u001b[34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m expect_async(exp, timeout)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kernmlops/.venv/lib/python3.12/site-packages/pexpect/expect.py:179\u001b[39m, in \u001b[36mExpecter.expect_loop\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    177\u001b[39m             timeout = end_time - time.time()\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EOF \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meof\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TIMEOUT \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.timeout(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kernmlops/.venv/lib/python3.12/site-packages/pexpect/expect.py:122\u001b[39m, in \u001b[36mExpecter.eof\u001b[39m\u001b[34m(self, err)\u001b[39m\n\u001b[32m    120\u001b[39m exc = EOF(msg)\n\u001b[32m    121\u001b[39m exc.__cause__ = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# in Python 3.x we can use \"raise exc from None\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[31mEOF\u001b[39m: End Of File (EOF). Exception style platform.\n<pexpect.pty_spawn.spawn object at 0x7291e548c800>\ncommand: /usr/bin/make\nargs: ['/usr/bin/make', 'docker']\nbuffer (last 100 chars): b''\nbefore (last 100 chars): b'ors.BenchmarkError: iperf3 server not responding\\r\\n\\r\\nmake: *** [Makefile:107: collect-data] Error 1\\r\\n'\nafter: <class 'pexpect.exceptions.EOF'>\nmatch: None\nmatch_index: None\nexitstatus: None\nflag_eof: True\npid: 50962\nchild_fd: 58\nclosed: False\ntimeout: None\ndelimiter: <class 'pexpect.exceptions.EOF'>\nlogfile: None\nlogfile_read: None\nlogfile_send: None\nmaxread: 2000\nignorecase: False\nsearchwindowsize: None\ndelaybeforesend: 0.05\ndelayafterclose: 0.1\ndelayafterterminate: 0.1\nsearcher: searcher_string:\n    0: b'Started benchmark'"
     ]
    }
   ],
   "source": [
    "# Test tcp_v4_rcv hook with iperf3 benchmark\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create collector with iperf configuration\n",
    "collect = Collector(\"./config/iperf.yaml\")\n",
    "\n",
    "# Start collection and run benchmark\n",
    "print(\"Starting iperf3 TCP benchmark...\")\n",
    "print(\"This will generate high-volume TCP traffic for 30 seconds\")\n",
    "collect.start_collection(None)\n",
    "\n",
    "# Wait for benchmark to complete\n",
    "data = collect.wait()\n",
    "\n",
    "# Analyze TCP traffic\n",
    "print(\"\\nAnalyzing TCP traffic from iperf3 benchmark:\")\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nTotal TCP events captured: {len(tcp_df)}\")\n",
    "\n",
    "# Analyze by process\n",
    "iperf_server = tcp_df.filter(pl.col(\"comm\").str.contains(\"iperf3.*-s\"))\n",
    "iperf_client = tcp_df.filter(pl.col(\"comm\").str.contains(\"iperf3\").and_(~pl.col(\"comm\").str.contains(\"-s\")))\n",
    "\n",
    "print(f\"\\nProcess breakdown:\")\n",
    "print(f\"- iperf3 server: {len(iperf_server)} events\")\n",
    "print(f\"- iperf3 client: {len(iperf_client)} events\")\n",
    "\n",
    "# Analyze port 5201 traffic (iperf3 default port)\n",
    "port_5201 = tcp_df.filter((pl.col(\"dport\") == 5201) | (pl.col(\"sport\") == 5201))\n",
    "print(f\"- Port 5201 traffic: {len(port_5201)} events\")\n",
    "\n",
    "# Branch distribution\n",
    "print(\"\\nTCP state distribution:\")\n",
    "branch_dist = tcp_df.group_by(\"branch_name\").agg([\n",
    "    pl.count().alias(\"count\"),\n",
    "    (pl.count() / len(tcp_df) * 100).alias(\"percentage\")\n",
    "]).sort(\"count\", descending=True)\n",
    "\n",
    "for row in branch_dist.head(10).iter_rows():\n",
    "    print(f\"- {row[0]}: {row[1]:,} events ({row[2]:.1f}%)\")\n",
    "\n",
    "# Connection analysis\n",
    "new_connections = tcp_df.filter(pl.col(\"branch_name\") == \"new_syn_recv\")\n",
    "print(f\"\\nNew TCP connections: {len(new_connections)}\")\n",
    "print(f\"Connections per stream: ~{len(new_connections) / 4:.0f}\")  # 4 parallel streams\n",
    "\n",
    "# Timeline visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Convert timestamps to seconds\n",
    "tcp_times = (tcp_df['uptime_timestamp'] - tcp_df['uptime_timestamp'].min()) / 1_000_000\n",
    "\n",
    "# Plot 1: Overall TCP events timeline\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.hist(tcp_times, bins=100, alpha=0.7, color='blue')\n",
    "plt.title('TCP Events Timeline - All Traffic')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Event Count')\n",
    "\n",
    "# Plot 2: Server vs Client events\n",
    "plt.subplot(3, 1, 2)\n",
    "server_times = (iperf_server['uptime_timestamp'] - tcp_df['uptime_timestamp'].min()) / 1_000_000\n",
    "client_times = (iperf_client['uptime_timestamp'] - tcp_df['uptime_timestamp'].min()) / 1_000_000\n",
    "\n",
    "plt.hist(server_times, bins=50, alpha=0.5, color='green', label='Server')\n",
    "plt.hist(client_times, bins=50, alpha=0.5, color='red', label='Client')\n",
    "plt.title('TCP Events - Server vs Client')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Event Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: TCP states over time\n",
    "plt.subplot(3, 1, 3)\n",
    "# Focus on main states\n",
    "main_states = ['entry', 'time_wait', 'socket_busy', 'new_syn_recv']\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "\n",
    "for state, color in zip(main_states, colors):\n",
    "    state_df = tcp_df.filter(pl.col(\"branch_name\") == state)\n",
    "    if len(state_df) > 0:\n",
    "        state_times = (state_df['uptime_timestamp'] - tcp_df['uptime_timestamp'].min()) / 1_000_000\n",
    "        plt.hist(state_times, bins=30, alpha=0.5, label=state, color=color)\n",
    "\n",
    "plt.title('TCP States Over Time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Event Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('iperf_tcp_analysis.png')\n",
    "plt.show()\n",
    "\n",
    "# Bandwidth estimation (rough)\n",
    "# Each TCP event represents some data transfer activity\n",
    "total_duration = (tcp_df['uptime_timestamp'].max() - tcp_df['uptime_timestamp'].min()) / 1_000_000\n",
    "events_per_second = len(tcp_df) / total_duration\n",
    "\n",
    "print(f\"\\nThroughput metrics:\")\n",
    "print(f\"- Total duration: {total_duration:.1f} seconds\")\n",
    "print(f\"- TCP events/second: {events_per_second:.0f}\")\n",
    "\n",
    "# Drop analysis\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "if len(drops) > 0:\n",
    "    print(f\"\\nDropped packets: {len(drops)}\")\n",
    "    drop_dist = drops.group_by(\"drop_reason_name\").len()\n",
    "    for row in drop_dist.iter_rows():\n",
    "        print(f\"- {row[0]}: {row[1]} drops\")\n",
    "else:\n",
    "    print(\"\\nNo dropped packets detected - excellent!\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- iperf3 successfully generated high-volume TCP traffic\")\n",
    "print(f\"- Captured detailed TCP state transitions\")\n",
    "print(f\"- Parallel streams created multiple concurrent connections\")\n",
    "print(f\"- Data suitable for TCP performance analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419c3644-76a6-4a25-8b77-8896bd9698f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17669    0 17669    0     0  90050      0 --:--:-- --:--:-- --:--:-- 90147\n",
      "shape: (5, 2)\n",
      "┌──────────────┬───────┐\n",
      "│ branch_name  ┆ count │\n",
      "│ ---          ┆ ---   │\n",
      "│ str          ┆ u32   │\n",
      "╞══════════════╪═══════╡\n",
      "│ entry        ┆ 114   │\n",
      "│ new_syn_recv ┆ 113   │\n",
      "│ time_wait    ┆ 113   │\n",
      "│ socket_busy  ┆ 7     │\n",
      "│ no_socket    ┆ 1     │\n",
      "└──────────────┴───────┘\n",
      "shape: (1, 2)\n",
      "┌──────────────────┬───────┐\n",
      "│ drop_reason_name ┆ count │\n",
      "│ ---              ┆ ---   │\n",
      "│ str              ┆ u32   │\n",
      "╞══════════════════╪═══════╡\n",
      "│ no_socket        ┆ 1     │\n",
      "└──────────────────┴───────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48740/3465806384.py:21: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
      "/tmp/ipykernel_48740/3465806384.py:25: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(drops.group_by(\"drop_reason_name\").count())\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "# New TCP Collector\n",
    "collect = Collector(\"./config/raw_overrides.yaml\")\n",
    "collect.start_collection()\n",
    "\n",
    "# Generate some TCP traffic\n",
    "!curl https://www.google.com > /dev/null\n",
    "!nc -l 8080 &  # Listen on port 8080\n",
    "!echo \"test\" | nc localhost 8080 \n",
    "\n",
    "data = collect.stop_collection()\n",
    "\n",
    "# Analyze TCP branches\n",
    "import polars as pl\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "# Show branch distribution\n",
    "print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Show drop reasons\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "print(drops.group_by(\"drop_reason_name\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e59363-0273-4ed1-bfd3-cabda586c325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (7, 2)\n",
      "┌──────────────┬───────┐\n",
      "│ branch_name  ┆ count │\n",
      "│ ---          ┆ ---   │\n",
      "│ str          ┆ u32   │\n",
      "╞══════════════╪═══════╡\n",
      "│ entry        ┆ 13727 │\n",
      "│ time_wait    ┆ 13711 │\n",
      "│ new_syn_recv ┆ 13706 │\n",
      "│ socket_busy  ┆ 1020  │\n",
      "│ no_socket    ┆ 16    │\n",
      "│ listen_state ┆ 10    │\n",
      "│ not_for_host ┆ 5     │\n",
      "└──────────────┴───────┘\n",
      "shape: (2, 2)\n",
      "┌──────────────────┬───────┐\n",
      "│ drop_reason_name ┆ count │\n",
      "│ ---              ┆ ---   │\n",
      "│ str              ┆ u32   │\n",
      "╞══════════════════╪═══════╡\n",
      "│ no_socket        ┆ 16    │\n",
      "│ not_specified    ┆ 5     │\n",
      "└──────────────────┴───────┘\n",
      "Total faults: 422246\n",
      "Major faults: 9293\n",
      "Major faults for page_fault app: 0\n",
      "shape: (18, 2)\n",
      "┌─────────────────┬─────┐\n",
      "│ comm            ┆ len │\n",
      "│ ---             ┆ --- │\n",
      "│ str             ┆ u32 │\n",
      "╞═════════════════╪═════╡\n",
      "│ python          ┆ 44  │\n",
      "│ dirname         ┆ 13  │\n",
      "│ ycsb            ┆ 46  │\n",
      "│ redis-cli       ┆ 107 │\n",
      "│ C1 CompilerThre ┆ 1   │\n",
      "│ …               ┆ …   │\n",
      "│ systemd-journal ┆ 2   │\n",
      "│ C2 CompilerThre ┆ 3   │\n",
      "│ tmcc.bin        ┆ 7   │\n",
      "│ expr            ┆ 64  │\n",
      "│ sshd            ┆ 90  │\n",
      "└─────────────────┴─────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48740/2535777981.py:10: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
      "/tmp/ipykernel_48740/2535777981.py:14: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(drops.group_by(\"drop_reason_name\").count())\n"
     ]
    }
   ],
   "source": [
    "collect = Collector(\"./config/redis_never.yaml\")\n",
    "collect.start_collection(None)\n",
    "data = collect.stop_collection()\n",
    "\n",
    "# Analyze TCP branches\n",
    "import polars as pl\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "# Show branch distribution\n",
    "print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Show drop reasons\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "print(drops.group_by(\"drop_reason_name\").count())\n",
    "\n",
    "# Analyze results\n",
    "import polars as pl\n",
    "df = pl.read_parquet(data[\"page_fault\"])\n",
    "# print(df.head())\n",
    "print(f\"Total faults: {len(df)}\")\n",
    "print(f\"Major faults: {df.filter(pl.col('is_major')).height}\")\n",
    "# Check fault patterns\n",
    "df_filtered = df.filter(\n",
    "    (pl.col('comm') == 'page_fault') & \n",
    "    (pl.col('is_major') == True)\n",
    ")\n",
    "print(f\"Major faults for page_fault app: {len(df_filtered)}\")\n",
    "\n",
    "major_summary = df.filter(pl.col('is_major')).group_by('comm').len()\n",
    "print(major_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1994657-5088-4a55-8c97-785073912778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collection with XSBench workload...\n",
      "Running XSBench benchmark...\n",
      "\n",
      "Analyzing TCP traffic from XSBench:\n",
      "\n",
      "TCP branch distribution:\n",
      "shape: (4, 2)\n",
      "┌──────────────┬─────┐\n",
      "│ branch_name  ┆ len │\n",
      "│ ---          ┆ --- │\n",
      "│ str          ┆ u32 │\n",
      "╞══════════════╪═════╡\n",
      "│ time_wait    ┆ 19  │\n",
      "│ new_syn_recv ┆ 19  │\n",
      "│ entry        ┆ 19  │\n",
      "│ socket_busy  ┆ 6   │\n",
      "└──────────────┴─────┘\n",
      "\n",
      "No dropped packets detected\n",
      "\n",
      "TCP activity by process:\n",
      "shape: (5, 2)\n",
      "┌─────────────────┬─────┐\n",
      "│ comm            ┆ len │\n",
      "│ ---             ┆ --- │\n",
      "│ str             ┆ u32 │\n",
      "╞═════════════════╪═════╡\n",
      "│ code-2901c5ac6d ┆ 21  │\n",
      "│ swapper/14      ┆ 15  │\n",
      "│ ZMQbg/IO/0      ┆ 14  │\n",
      "│ jupyter-noteboo ┆ 7   │\n",
      "│ sshd            ┆ 6   │\n",
      "└─────────────────┴─────┘\n",
      "\n",
      "No direct TCP traffic from XSBench process detected\n"
     ]
    }
   ],
   "source": [
    "# Create collector with XSBench configuration\n",
    "collect = Collector(\"./config/xsbench.yaml\")\n",
    "\n",
    "# Start collection and run XSBench\n",
    "print(\"Starting collection with XSBench workload...\")\n",
    "collect.start_collection(None)\n",
    "\n",
    "# Wait for XSBench to complete\n",
    "print(\"Running XSBench benchmark...\")\n",
    "data = collect.stop_collection()\n",
    "\n",
    "# Analyze TCP traffic generated by XSBench\n",
    "print(\"\\nAnalyzing TCP traffic from XSBench:\")\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "# Show branch distribution\n",
    "print(\"\\nTCP branch distribution:\")\n",
    "print(tcp_df.group_by(\"branch_name\").len().sort(\"len\", descending=True))\n",
    "\n",
    "# Show drop reasons if any\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "if len(drops) > 0:\n",
    "    print(\"\\nDropped packets:\")\n",
    "    print(drops.group_by(\"drop_reason_name\").len())\n",
    "else:\n",
    "    print(\"\\nNo dropped packets detected\")\n",
    "\n",
    "# Show process-specific TCP activity\n",
    "print(\"\\nTCP activity by process:\")\n",
    "process_tcp = tcp_df.group_by(\"comm\").len().sort(\"len\", descending=True).head(10)\n",
    "print(process_tcp)\n",
    "\n",
    "# Check for XSBench-specific activity\n",
    "xsbench_traffic = tcp_df.filter(pl.col(\"comm\").str.contains(\"XSBench\"))\n",
    "if len(xsbench_traffic) > 0:\n",
    "    print(f\"\\nXSBench generated {len(xsbench_traffic)} TCP events\")\n",
    "else:\n",
    "    print(\"\\nNo direct TCP traffic from XSBench process detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c72431-e9f9-401e-a3a6-1e69dd1063f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Page Fault Collector\n",
    "collect = Collector(\"./config/raw_overrides.yaml\")\n",
    "log = open(\"page_fault_log.txt\", \"bw\")\n",
    "    \n",
    "collect.start_collection(logfile=log)\n",
    "\n",
    "# Run a program that causes exactly 1 page fault\n",
    "!sudo ./page_fault\n",
    "\n",
    "data = collect.stop_collection()\n",
    "log.close()\n",
    "\n",
    "# Check what was collected\n",
    "# print(\"Available keys:\", data.keys())\n",
    "# print(data)\n",
    "\n",
    "# Read the log to see if there were errors\n",
    "# with open(\"page_fault_log.txt\", \"r\") as f:\n",
    "#    print(\"Log contents:\", f.read())\n",
    "\n",
    "# Analyze results\n",
    "import polars as pl\n",
    "df = pl.read_parquet(data[\"page_fault\"])\n",
    "# print(df.head())\n",
    "print(f\"Total faults: {len(df)}\")\n",
    "print(f\"Major faults: {df.filter(pl.col('is_major')).height}\")\n",
    "# Check fault patterns\n",
    "df_filtered = df.filter(\n",
    "    (pl.col('comm') == 'page_fault') & \n",
    "    (pl.col('is_major') == True)\n",
    ")\n",
    "print(f\"Major faults for page_fault app: {len(df_filtered)}\")\n",
    "\n",
    "major_summary = df.filter(pl.col('is_major')).group_by('comm').len()\n",
    "print(major_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6f636-6e03-4a04-bc5d-6b4cff6c9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = Collector(\"./config/raw_overrides.yaml\")\n",
    "# This creates a raw collector, I suggest looking into this file to learn more\n",
    "\n",
    "w = open(\"hello.txt\", \"bw\")\n",
    "collect.start_collection(logfile=w)\n",
    "print(\"Collection has started\")\n",
    "# Start collection\n",
    "\n",
    "f = open(\"blah.txt\", \"w\")\n",
    "bench_test = subprocess.Popen([\"cat\", \"defaults.yaml\"], stdout=f)\n",
    "bench_test.wait()\n",
    "# Run benchmark application\n",
    "\n",
    "# Run a program that causes page faults\n",
    "!python -c \"import numpy as np; a = np.zeros((1000, 1000, 100))\" & echo $!\n",
    "!ps -a\n",
    "\n",
    "print(\"Exit application\")\n",
    "raw_coll_info = collect.stop_collection()\n",
    "print(raw_coll_info)\n",
    "# Stop the Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68efc94-d113-4ea6-afd5-098f2f509ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze page fault results here\n",
    "import polars as pl\n",
    "df = pl.read_parquet(raw_coll_info[\"page_fault\"])\n",
    "print(df.head())\n",
    "print(f\"Total faults: {len(df)}\")\n",
    "print(f\"Major faults: {df.filter(pl.col('is_major')).height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b719918-a008-4385-8c0b-59fd99faa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = Collector(\"./config/start_overrides.yaml\")\n",
    "# This is a simple redis benchmark config\n",
    "\n",
    "w = open(\"hello.txt\", \"bw\")\n",
    "collect.start_collection(logfile=w)\n",
    "# Start collection\n",
    "\n",
    "start_coll_info = collect.wait()\n",
    "#Wait for collector to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77c577-d4bd-4c97-86dd-f69388c796de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_coll_info.keys())\n",
    "print(start_coll_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb351660-009f-4c1f-9cd6-8d385159c023",
   "metadata": {},
   "source": [
    "Now let's try to examine some of the system information from this.\n",
    "I use polars, you can use whatever you like as far as data frames go, so long as they can read parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f164d85-48c6-4a1e-992c-28c17ac35fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_parquet(raw_coll_info[\"process_trace\"])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9258bc08-a748-4cb4-9ad8-466aeec46b6b",
   "metadata": {},
   "source": [
    "As a very easy excercise we could filter out the processes that are created during the life of the program and give them the name of the last exec call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b0c2e-8e9f-4422-b729-be8c0a2ae1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_parquet(raw_coll_info[\"process_trace\"])\n",
    "\n",
    "def filter_process_trace(process_trace_df: pl.DataFrame) -> pl.DataFrame :\n",
    "    df = process_trace_df\n",
    "    # Filter just the processes\n",
    "    df = df.filter(pl.col(\"tgid\") == pl.col(\"pid\")).drop(\"collection_id\")\n",
    "\n",
    "    # Find the last name of each process\n",
    "    start_df = df.sort(pl.col(\"ts_ns\"), descending = True)\n",
    "    helper_dict = {}\n",
    "    for row in start_df.iter_rows():\n",
    "        pid = row[0]\n",
    "        comm = row[3]\n",
    "        if pid in helper_dict.keys() or comm == \"\": \n",
    "            continue\n",
    "        helper_dict[pid] = comm\n",
    "\n",
    "    # Separate the start and end\n",
    "    full_df = start_df.with_columns(pl.col(\"pid\").map_elements(lambda x : helper_dict.get(x, \"\"), return_dtype=str).alias(\"full_name\"))\n",
    "    full_df = full_df.drop([\"tgid\", \"name\"])\n",
    "    start_df = full_df.filter(pl.col(\"cap_type\") == \"start\").rename({\"ts_ns\": \"start_ns\"}).drop(\"cap_type\")\n",
    "    end_df = full_df.filter(pl.col(\"cap_type\") == \"end\").rename({\"ts_ns\": \"end_ns\"}).drop([\"cap_type\", \"full_name\"])\n",
    "\n",
    "    # Join them to get the process table\n",
    "    return start_df.join(end_df, \"pid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247cd05-d451-48b0-a83f-a2404f9f5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_process_trace(pl.read_parquet(raw_coll_info[\"process_trace\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705e626-63c6-42da-9d6e-4a9f916e8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_process_trace(pl.read_parquet(start_coll_info[\"process_trace\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fa599-56a0-4773-be03-7629f6d337a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def filter_process_trace(process_trace_df: pl.DataFrame) -> pl.DataFrame :\n",
    "    df = process_trace_df\n",
    "    # Filter just the processes\n",
    "    df = df.filter(pl.col(\"tgid\") == pl.col(\"pid\")).drop(\"collection_id\")\n",
    "\n",
    "    # Find the last name of each process\n",
    "    start_df = df.sort(pl.col(\"ts_ns\"), descending = True)\n",
    "    helper_dict = {}\n",
    "    for row in start_df.iter_rows():\n",
    "        pid = row[0]\n",
    "        comm = row[3]\n",
    "        if pid in helper_dict.keys() or comm == \"\": \n",
    "            continue\n",
    "        helper_dict[pid] = comm\n",
    "\n",
    "    # Separate the start and end\n",
    "    full_df = start_df.with_columns(pl.col(\"pid\").map_elements(lambda x : helper_dict.get(x, \"\"), return_dtype=str).alias(\"full_name\"))\n",
    "    full_df = full_df.drop([\"tgid\", \"name\"])\n",
    "    start_df = full_df.filter(pl.col(\"cap_type\") == \"start\").rename({\"ts_ns\": \"start_ns\"}).drop(\"cap_type\")\n",
    "    end_df = full_df.filter(pl.col(\"cap_type\") == \"end\").rename({\"ts_ns\": \"end_ns\"}).drop([\"cap_type\", \"full_name\"])\n",
    "\n",
    "    # Join them to get the process table\n",
    "    combined_df = start_df.join(end_df, \"pid\")\n",
    "    return combined_df.with_columns((pl.col(\"end_ns\") - pl.col(\"start_ns\")).alias(\"duration\"))\n",
    "\n",
    "def process_trace_start_end_ts(process_trace_df: pl.DataFrame, proc_name: str, index: int) ->(int, int, int):\n",
    "    trace_df = filter_process_trace(process_trace_df).sort(pl.col(\"start_ns\"))\n",
    "    df = trace_df.filter(pl.col(\"full_name\") == proc_name)\n",
    "    print(df)\n",
    "    df = df[index]\n",
    "    pid = df[\"pid\"][0]\n",
    "    start_ns = df[\"start_ns\"][0]\n",
    "    end_ns = df[\"end_ns\"][0]\n",
    "    return pid, start_ns, end_ns\n",
    "\n",
    "def clean_rss_pid(rss_df: pl.DataFrame, pid: int) -> pl.DataFrame:\n",
    "    df = rss_df.drop([\"pid\", \"collection_id\"]).sort(pl.col(\"ts_ns\"))\n",
    "    df = df.filter(pl.col(\"tgid\") == pid)\n",
    "    df = df.with_columns(pl.when(pl.col(\"member\") == \"MM_FILEPAGES\")\n",
    "                     .then(pl.col(\"count\"))\n",
    "                     .otherwise(None)\n",
    "                     .fill_null(strategy=\"forward\")\n",
    "                     .fill_null(strategy=\"backward\")\n",
    "                     .alias(\"file\"))\n",
    "    df = df.with_columns(pl.when(pl.col(\"member\") == \"MM_ANONPAGES\")\n",
    "                     .then(pl.col(\"count\"))\n",
    "                     .otherwise(None)\n",
    "                     .fill_null(strategy=\"forward\")\n",
    "                     .fill_null(strategy=\"backward\")\n",
    "                     .alias(\"anon\"))\n",
    "    df = df.with_columns(pl.when(pl.col(\"member\") == \"MM_SWAPENTS\")\n",
    "                     .then(pl.col(\"count\"))\n",
    "                     .otherwise(None)\n",
    "                     .fill_null(strategy=\"forward\")\n",
    "                     .fill_null(strategy=\"backward\")\n",
    "                     .alias(\"swap\"))\n",
    "    df = df.drop([\"member\", \"count\"])\n",
    "    zero_df = pl.DataFrame({\"tgid\": pid, \"ts_ns\" : -1, \"file\" : 0, \"anon\": 0, \"swap\": 0})\n",
    "    df = pl.concat([df, zero_df]).sort(\"ts_ns\")\n",
    "    df = df.fill_null(strategy=\"forward\")\n",
    "    df = df.filter(pl.col(\"ts_ns\") >= 0)\n",
    "    df = df.with_columns((pl.col(\"file\") + pl.col(\"anon\") + pl.col(\"swap\")).alias(\"count\"))\n",
    "    df = df.drop([\"file\", \"anon\", \"swap\"])\n",
    "    return df\n",
    "\n",
    "def filter_rss_with_ts(rss_trace_df: pl.DataFrame, start: int, end: int):\n",
    "    print(start, end)\n",
    "    new_frame_dict = {}\n",
    "    for column_name in rss_trace_df.columns:\n",
    "        new_frame_dict[column_name] = [None, None]\n",
    "    new_frame_dict[\"ts_ns\"] = [start, end]\n",
    "    df = rss_trace_df.vstack(pl.DataFrame(new_frame_dict))\n",
    "    df = df.sort(pl.col(\"ts_ns\")).fill_null(strategy=\"forward\").fill_null(strategy=\"backward\")\n",
    "    return df.filter(pl.col(\"ts_ns\").is_between(start, end, closed='both'))\n",
    "\n",
    "def get_proper_rss(proc_path: Path, rss_path: Path, rss_name: str, rss_ind: int, runner_name: str, runner_ind: int, tag:str):\n",
    "    proc_trace_df = pl.read_parquet(proc_path)\n",
    "    rss_df = pl.read_parquet(rss_path)\n",
    "\n",
    "    _, start, end = process_trace_start_end_ts(proc_trace_df, runner_name, runner_ind)\n",
    "    pid, _, _ = process_trace_start_end_ts(proc_trace_df, rss_name, rss_ind)\n",
    "    clean_rss_df = filter_rss_with_ts(clean_rss_pid(rss_df, pid), start, end)\n",
    "    return clean_rss_df.with_columns((pl.col(\"ts_ns\") - pl.min(\"ts_ns\")).alias(\"norm_ts_ns\")).with_columns(pl.lit(tag).alias('policy'))\n",
    "\n",
    "from pathlib import Path\n",
    "from plotnine import ggplot, aes, geom_line, geom_point, labs\n",
    "\n",
    "def create_graph(inputs: [(str, dict[str, Path])], proc_tag: str, proc_ind: int, time_proc_tag: str, time_proc_index: int, title: str) -> None:\n",
    "    df = pl.DataFrame()\n",
    "    for (tag, filedict) in inputs:\n",
    "        append_df = get_proper_rss(filedict[\"process_trace\"],\n",
    "                                   filedict[\"mm_rss_stat\"],\n",
    "                                   proc_tag, proc_ind,\n",
    "                                   time_proc_tag,\n",
    "                                   time_proc_index,\n",
    "                                   tag).drop([\"tgid\", \"ts_ns\"])\n",
    "        df = pl.concat([df, append_df])\n",
    "    df = df.with_columns((pl.col(\"norm_ts_ns\") / (10**9)/ 60).alias(\"norm_ts_mins\"))\n",
    "    plt0 = (ggplot(df)\n",
    "            + aes(\"norm_ts_mins\", y=\"count\", colour=\"policy\")\n",
    "            + geom_point()\n",
    "            + geom_line()\n",
    "            + labs(x=\"Time (mins)\",\n",
    "                   y=\"4kB Pages\",\n",
    "                   title=title)\n",
    "           )\n",
    "    return plt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94874478-33b2-4c67-989c-dc98cfc529f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = Collector(\"./config/redis_never.yaml\")\n",
    "collect.start_collection(None)\n",
    "redis_never_info = collect.wait()\n",
    "collect = Collector(\"./config/redis_madvise.yaml\")\n",
    "collect.start_collection(None)\n",
    "redis_madvise_info = collect.wait()\n",
    "collect = Collector(\"./config/redis_always.yaml\")\n",
    "collect.start_collection(None)\n",
    "redis_always_info = collect.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcfd78-4c61-4c00-b3a2-5fb4d8a415cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "plt = create_graph([(\"4k\", redis_never_info),\n",
    "                    (\"madvise\", redis_madvise_info),\n",
    "                    (\"thp\", redis_always_info)],\n",
    "                   \"redis-server\", 0, \n",
    "                   \"redis-server\", 0,\n",
    "                   \"Redis driven by YCSB with Insertions and Deletes using Jemalloc\")\n",
    "plt.save(\"deletes-redis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28cd4d-c553-467e-83e3-85cf2a64bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"./deletes-redis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad914c-bd29-4e80-a5c9-f37cb88f14d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
