{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da464b6-89fc-4ca2-9290-f3bebbe2dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make docker-image > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62bef45-339e-44c1-8309-874c236ed23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CONTAINER_CMD=\"bash -lc 'make install-ycsb\" make docker > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a1beeb-3882-4dd0-91c7-aa98c303c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pexpect\n",
    "import os\n",
    "import time\n",
    "\n",
    "\"\"\" Collector class has helper methods to interact with kermit\"\"\"\n",
    "class Collector: \n",
    "    def __init__(self, config: Path):\n",
    "        self.env = os.environ.copy()\n",
    "        self.env[\"INTERACTIVE\"] = \"it\"\n",
    "        self.env[\"CONTAINER_CMD\"] = f\"bash -lc 'KERNMLOPS_CONFIG_FILE={config} make collect-data'\"\n",
    "        self.collect_process : pexpect.spawn | None = None\n",
    "\n",
    "    def start_collection(self, logfile=None):\n",
    "        self.collect_process = pexpect.spawn(\"make docker\", env=self.env, timeout=None, logfile=logfile)\n",
    "        self.collect_process.expect_exact([\"Started benchmark\"])\n",
    "\n",
    "    def _after_run_generate_file_data() -> dict[str, list[Path]]:\n",
    "        start_path : Path = Path(\"./data\")\n",
    "        list_of_collect_id_dirs = start_path.glob(\"*/*/*\")\n",
    "        latest_collect_id = max(list_of_collect_id_dirs, key=os.path.getctime)\n",
    "        list_of_files = latest_collect_id.glob(\"*.*.parquet\")\n",
    "        output = {}\n",
    "        for f in list_of_files:\n",
    "            index = str(f).removeprefix(str(f.parent) + \"/\").split(\".\")[0]\n",
    "            if index not in output.keys():\n",
    "                output[index] = []\n",
    "            output[index].append(f)\n",
    "        return output\n",
    "        \n",
    "    def wait(self) -> int:\n",
    "        if self.collect_process is None:\n",
    "            return\n",
    "        self.collect_process.expect([pexpect.EOF])\n",
    "        self.collect_process.wait()\n",
    "        return Collector._after_run_generate_file_data()\n",
    "        \n",
    "    def stop_collection(self):\n",
    "        if self.collect_process is None:\n",
    "            return\n",
    "        self.collect_process.sendline(\"END\")\n",
    "        return self.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5874f9a-d168-446b-9a52-ec5b49f7c35f",
   "metadata": {},
   "source": [
    "There are two ways to run kermit:\n",
    "- With the raw config\n",
    "- With a pre-programmed benchmark config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4841813f-c1fd-49d6-8092-89b08117e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing iperf3 installation...\n",
      "iperf3 is already installed\n",
      "\n",
      "Starting iperf3 server on port 5555...\n",
      "Testing server connection...\n",
      "✓ iperf3 is working!\n",
      "\n",
      "Running benchmark with kernmlops...\n",
      "\n",
      "✓ Success! Captured 3,842 TCP events\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pkill', '-f', 'iperf3'], returncode=0, stdout=b'', stderr=b'')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple iperf3 Test - Docker Friendly Version\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# First, let's manually test if iperf3 works\n",
    "print(\"Testing iperf3 installation...\")\n",
    "\n",
    "# Install iperf3 if needed\n",
    "install_result = subprocess.run([\"which\", \"iperf3\"], capture_output=True)\n",
    "if install_result.returncode != 0:\n",
    "    print(\"Installing iperf3...\")\n",
    "    subprocess.run([\"apt-get\", \"update\"], capture_output=True)\n",
    "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"iperf3\"], capture_output=True)\n",
    "    print(\"iperf3 installed!\")\n",
    "else:\n",
    "    print(\"iperf3 is already installed\")\n",
    "\n",
    "# Kill any existing iperf3 processes\n",
    "subprocess.run([\"pkill\", \"-f\", \"iperf3\"], capture_output=True)\n",
    "time.sleep(1)\n",
    "\n",
    "# Start iperf3 server manually\n",
    "print(\"\\nStarting iperf3 server on port 5555...\")\n",
    "server = subprocess.Popen(\n",
    "    [\"iperf3\", \"-s\", \"-p\", \"5555\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "time.sleep(3)\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing server connection...\")\n",
    "test = subprocess.run(\n",
    "    [\"iperf3\", \"-c\", \"127.0.0.1\", \"-p\", \"5555\", \"-t\", \"1\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if test.returncode == 0:\n",
    "    print(\"✓ iperf3 is working!\")\n",
    "    \n",
    "    # Now run actual benchmark with kernmlops\n",
    "    print(\"\\nRunning benchmark with kernmlops...\")\n",
    "    \n",
    "    # Use minimal config for Docker\n",
    "    collect = Collector(\"./config/iperf_docker.yaml\")\n",
    "    \n",
    "    try:\n",
    "        collect.start_collection(None)\n",
    "        data = collect.wait()\n",
    "        \n",
    "        import polars as pl\n",
    "        tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "        print(f\"\\n✓ Success! Captured {len(tcp_df):,} TCP events\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Benchmark failed: {e}\")\n",
    "        print(\"\\nTry using the alternative method below...\")\n",
    "        \n",
    "else:\n",
    "    print(\"✗ iperf3 server test failed\")\n",
    "    print(f\"Error: {test.stderr}\")\n",
    "\n",
    "# Cleanup\n",
    "server.terminate()\n",
    "subprocess.run([\"pkill\", \"-f\", \"iperf3\"], capture_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4ad3c7-0c6d-4da6-bde8-9d2bec08be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Process breakdown:\n",
      "- iperf3 server: 0 events\n",
      "- iperf3 client: 2428 events\n",
      "- Port 5201 traffic: 0 events\n",
      "\n",
      "TCP state distribution:\n",
      "- entry: 1,240 events (32.3%)\n",
      "- new_syn_recv: 1,235 events (32.1%)\n",
      "- time_wait: 1,235 events (32.1%)\n",
      "- socket_busy: 125 events (3.3%)\n",
      "- no_socket: 5 events (0.1%)\n",
      "- listen_state: 2 events (0.1%)\n",
      "\n",
      "New TCP connections: 1235\n",
      "Connections per stream: ~309\n",
      "\n",
      "Dropped packets: 5\n",
      "- no_socket: 5 drops\n",
      "shape: (6, 2)\n",
      "┌──────────────┬───────┐\n",
      "│ branch_name  ┆ count │\n",
      "│ ---          ┆ ---   │\n",
      "│ str          ┆ u32   │\n",
      "╞══════════════╪═══════╡\n",
      "│ entry        ┆ 1240  │\n",
      "│ time_wait    ┆ 1235  │\n",
      "│ new_syn_recv ┆ 1235  │\n",
      "│ socket_busy  ┆ 125   │\n",
      "│ no_socket    ┆ 5     │\n",
      "│ listen_state ┆ 2     │\n",
      "└──────────────┴───────┘\n",
      "shape: (1, 2)\n",
      "┌──────────────────┬───────┐\n",
      "│ drop_reason_name ┆ count │\n",
      "│ ---              ┆ ---   │\n",
      "│ str              ┆ u32   │\n",
      "╞══════════════════╪═══════╡\n",
      "│ no_socket        ┆ 5     │\n",
      "└──────────────────┴───────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56029/414641187.py:18: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  pl.count().alias(\"count\"),\n",
      "/tmp/ipykernel_56029/414641187.py:19: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  (pl.count() / len(tcp_df) * 100).alias(\"percentage\")\n",
      "/tmp/ipykernel_56029/414641187.py:41: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
      "/tmp/ipykernel_56029/414641187.py:45: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(drops.group_by(\"drop_reason_name\").count())\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analyze by process\n",
    "iperf_server = tcp_df.filter(pl.col(\"comm\").str.contains(\"iperf3.*-s\"))\n",
    "iperf_client = tcp_df.filter(pl.col(\"comm\").str.contains(\"iperf3\").and_(~pl.col(\"comm\").str.contains(\"-s\")))\n",
    "\n",
    "print(f\"\\nProcess breakdown:\")\n",
    "print(f\"- iperf3 server: {len(iperf_server)} events\")\n",
    "print(f\"- iperf3 client: {len(iperf_client)} events\")\n",
    "\n",
    "# Analyze port 5201 traffic (iperf3 default port)\n",
    "port_5201 = tcp_df.filter((pl.col(\"dport\") == 5201) | (pl.col(\"sport\") == 5201))\n",
    "print(f\"- Port 5201 traffic: {len(port_5201)} events\")\n",
    "\n",
    "# Branch distribution\n",
    "print(\"\\nTCP state distribution:\")\n",
    "branch_dist = tcp_df.group_by(\"branch_name\").agg([\n",
    "    pl.count().alias(\"count\"),\n",
    "    (pl.count() / len(tcp_df) * 100).alias(\"percentage\")\n",
    "]).sort(\"count\", descending=True)\n",
    "\n",
    "for row in branch_dist.head(10).iter_rows():\n",
    "    print(f\"- {row[0]}: {row[1]:,} events ({row[2]:.1f}%)\")\n",
    "\n",
    "# Connection analysis\n",
    "new_connections = tcp_df.filter(pl.col(\"branch_name\") == \"new_syn_recv\")\n",
    "print(f\"\\nNew TCP connections: {len(new_connections)}\")\n",
    "print(f\"Connections per stream: ~{len(new_connections) / 4:.0f}\")  # 4 parallel streams\n",
    "\n",
    "# Drop analysis\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "if len(drops) > 0:\n",
    "    print(f\"\\nDropped packets: {len(drops)}\")\n",
    "    drop_dist = drops.group_by(\"drop_reason_name\").len()\n",
    "    for row in drop_dist.iter_rows():\n",
    "        print(f\"- {row[0]}: {row[1]} drops\")\n",
    "else:\n",
    "    print(\"\\nNo dropped packets detected - excellent!\")\n",
    "\n",
    "# Show branch distribution\n",
    "print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Show drop reasons\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "print(drops.group_by(\"drop_reason_name\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419c3644-76a6-4a25-8b77-8896bd9698f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17459    0 17459    0     0  91475      0 --:--:-- --:--:-- --:--:-- 91889\n",
      "shape: (5, 2)\n",
      "┌──────────────┬───────┐\n",
      "│ branch_name  ┆ count │\n",
      "│ ---          ┆ ---   │\n",
      "│ str          ┆ u32   │\n",
      "╞══════════════╪═══════╡\n",
      "│ entry        ┆ 50    │\n",
      "│ new_syn_recv ┆ 49    │\n",
      "│ time_wait    ┆ 49    │\n",
      "│ socket_busy  ┆ 11    │\n",
      "│ no_socket    ┆ 1     │\n",
      "└──────────────┴───────┘\n",
      "shape: (1, 2)\n",
      "┌──────────────────┬───────┐\n",
      "│ drop_reason_name ┆ count │\n",
      "│ ---              ┆ ---   │\n",
      "│ str              ┆ u32   │\n",
      "╞══════════════════╪═══════╡\n",
      "│ no_socket        ┆ 1     │\n",
      "└──────────────────┴───────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56029/3465806384.py:21: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
      "/tmp/ipykernel_56029/3465806384.py:25: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(drops.group_by(\"drop_reason_name\").count())\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "# New TCP Collector\n",
    "collect = Collector(\"./config/raw_overrides.yaml\")\n",
    "collect.start_collection()\n",
    "\n",
    "# Generate some TCP traffic\n",
    "!curl https://www.google.com > /dev/null\n",
    "!nc -l 8080 &  # Listen on port 8080\n",
    "!echo \"test\" | nc localhost 8080 \n",
    "\n",
    "data = collect.stop_collection()\n",
    "\n",
    "# Analyze TCP branches\n",
    "import polars as pl\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "# Show branch distribution\n",
    "print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Show drop reasons\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "print(drops.group_by(\"drop_reason_name\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e59363-0273-4ed1-bfd3-cabda586c325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (7, 2)\n",
      "┌──────────────┬───────┐\n",
      "│ branch_name  ┆ count │\n",
      "│ ---          ┆ ---   │\n",
      "│ str          ┆ u32   │\n",
      "╞══════════════╪═══════╡\n",
      "│ entry        ┆ 13360 │\n",
      "│ time_wait    ┆ 13342 │\n",
      "│ new_syn_recv ┆ 13337 │\n",
      "│ socket_busy  ┆ 1046  │\n",
      "│ no_socket    ┆ 18    │\n",
      "│ listen_state ┆ 6     │\n",
      "│ not_for_host ┆ 5     │\n",
      "└──────────────┴───────┘\n",
      "shape: (2, 2)\n",
      "┌──────────────────┬───────┐\n",
      "│ drop_reason_name ┆ count │\n",
      "│ ---              ┆ ---   │\n",
      "│ str              ┆ u32   │\n",
      "╞══════════════════╪═══════╡\n",
      "│ not_specified    ┆ 5     │\n",
      "│ no_socket        ┆ 18    │\n",
      "└──────────────────┴───────┘\n",
      "Total faults: 421082\n",
      "Major faults: 9254\n",
      "Major faults for page_fault app: 0\n",
      "shape: (19, 2)\n",
      "┌─────────────────┬──────┐\n",
      "│ comm            ┆ len  │\n",
      "│ ---             ┆ ---  │\n",
      "│ str             ┆ u32  │\n",
      "╞═════════════════╪══════╡\n",
      "│ java            ┆ 8749 │\n",
      "│ python          ┆ 44   │\n",
      "│ redis-cli       ┆ 106  │\n",
      "│ sshd            ┆ 57   │\n",
      "│ uname           ┆ 15   │\n",
      "│ …               ┆ …    │\n",
      "│ systemd-journal ┆ 1    │\n",
      "│ ls              ┆ 35   │\n",
      "│ debian-sa1      ┆ 7    │\n",
      "│ ycsb            ┆ 46   │\n",
      "│ C2 CompilerThre ┆ 3    │\n",
      "└─────────────────┴──────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56029/2535777981.py:10: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
      "/tmp/ipykernel_56029/2535777981.py:14: DeprecationWarning: `GroupBy.count` was renamed; use `GroupBy.len` instead\n",
      "  print(drops.group_by(\"drop_reason_name\").count())\n"
     ]
    }
   ],
   "source": [
    "collect = Collector(\"./config/redis_never.yaml\")\n",
    "collect.start_collection(None)\n",
    "data = collect.stop_collection()\n",
    "\n",
    "# Analyze TCP branches\n",
    "import polars as pl\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "# Show branch distribution\n",
    "print(tcp_df.group_by(\"branch_name\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Show drop reasons\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "print(drops.group_by(\"drop_reason_name\").count())\n",
    "\n",
    "# Analyze results\n",
    "import polars as pl\n",
    "df = pl.read_parquet(data[\"page_fault\"])\n",
    "# print(df.head())\n",
    "print(f\"Total faults: {len(df)}\")\n",
    "print(f\"Major faults: {df.filter(pl.col('is_major')).height}\")\n",
    "# Check fault patterns\n",
    "df_filtered = df.filter(\n",
    "    (pl.col('comm') == 'page_fault') & \n",
    "    (pl.col('is_major') == True)\n",
    ")\n",
    "print(f\"Major faults for page_fault app: {len(df_filtered)}\")\n",
    "\n",
    "major_summary = df.filter(pl.col('is_major')).group_by('comm').len()\n",
    "print(major_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1994657-5088-4a55-8c97-785073912778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collection with XSBench workload...\n",
      "Running XSBench benchmark...\n",
      "\n",
      "Analyzing TCP traffic from XSBench:\n",
      "\n",
      "TCP branch distribution:\n",
      "shape: (4, 2)\n",
      "┌──────────────┬─────┐\n",
      "│ branch_name  ┆ len │\n",
      "│ ---          ┆ --- │\n",
      "│ str          ┆ u32 │\n",
      "╞══════════════╪═════╡\n",
      "│ entry        ┆ 379 │\n",
      "│ new_syn_recv ┆ 379 │\n",
      "│ time_wait    ┆ 379 │\n",
      "│ socket_busy  ┆ 103 │\n",
      "└──────────────┴─────┘\n",
      "\n",
      "No dropped packets detected\n",
      "\n",
      "TCP activity by process:\n",
      "shape: (7, 2)\n",
      "┌─────────────────┬─────┐\n",
      "│ comm            ┆ len │\n",
      "│ ---             ┆ --- │\n",
      "│ str             ┆ u32 │\n",
      "╞═════════════════╪═════╡\n",
      "│ jupyter-noteboo ┆ 492 │\n",
      "│ sshd            ┆ 331 │\n",
      "│ code-2901c5ac6d ┆ 193 │\n",
      "│ node            ┆ 141 │\n",
      "│ swapper/13      ┆ 66  │\n",
      "│ ZMQbg/IO/0      ┆ 14  │\n",
      "│ swapper/14      ┆ 3   │\n",
      "└─────────────────┴─────┘\n",
      "\n",
      "No direct TCP traffic from XSBench process detected\n"
     ]
    }
   ],
   "source": [
    "# Create collector with XSBench configuration\n",
    "collect = Collector(\"./config/xsbench.yaml\")\n",
    "\n",
    "# Start collection and run XSBench\n",
    "print(\"Starting collection with XSBench workload...\")\n",
    "collect.start_collection(None)\n",
    "\n",
    "# Wait for XSBench to complete\n",
    "print(\"Running XSBench benchmark...\")\n",
    "data = collect.stop_collection()\n",
    "\n",
    "# Analyze TCP traffic generated by XSBench\n",
    "print(\"\\nAnalyzing TCP traffic from XSBench:\")\n",
    "tcp_df = pl.read_parquet(data[\"tcp_v4_rcv\"][0])\n",
    "\n",
    "# Show branch distribution\n",
    "print(\"\\nTCP branch distribution:\")\n",
    "print(tcp_df.group_by(\"branch_name\").len().sort(\"len\", descending=True))\n",
    "\n",
    "# Show drop reasons if any\n",
    "drops = tcp_df.filter(pl.col(\"drop_reason\") > 0)\n",
    "if len(drops) > 0:\n",
    "    print(\"\\nDropped packets:\")\n",
    "    print(drops.group_by(\"drop_reason_name\").len())\n",
    "else:\n",
    "    print(\"\\nNo dropped packets detected\")\n",
    "\n",
    "# Show process-specific TCP activity\n",
    "print(\"\\nTCP activity by process:\")\n",
    "process_tcp = tcp_df.group_by(\"comm\").len().sort(\"len\", descending=True).head(10)\n",
    "print(process_tcp)\n",
    "\n",
    "# Check for XSBench-specific activity\n",
    "xsbench_traffic = tcp_df.filter(pl.col(\"comm\").str.contains(\"XSBench\"))\n",
    "if len(xsbench_traffic) > 0:\n",
    "    print(f\"\\nXSBench generated {len(xsbench_traffic)} TCP events\")\n",
    "else:\n",
    "    print(\"\\nNo direct TCP traffic from XSBench process detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c72431-e9f9-401e-a3a6-1e69dd1063f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Page Fault Collector\n",
    "collect = Collector(\"./config/raw_overrides.yaml\")\n",
    "log = open(\"page_fault_log.txt\", \"bw\")\n",
    "    \n",
    "collect.start_collection(logfile=log)\n",
    "\n",
    "# Run a program that causes exactly 1 page fault\n",
    "!sudo ./page_fault\n",
    "\n",
    "data = collect.stop_collection()\n",
    "log.close()\n",
    "\n",
    "# Check what was collected\n",
    "# print(\"Available keys:\", data.keys())\n",
    "# print(data)\n",
    "\n",
    "# Read the log to see if there were errors\n",
    "# with open(\"page_fault_log.txt\", \"r\") as f:\n",
    "#    print(\"Log contents:\", f.read())\n",
    "\n",
    "# Analyze results\n",
    "import polars as pl\n",
    "df = pl.read_parquet(data[\"page_fault\"])\n",
    "# print(df.head())\n",
    "print(f\"Total faults: {len(df)}\")\n",
    "print(f\"Major faults: {df.filter(pl.col('is_major')).height}\")\n",
    "# Check fault patterns\n",
    "df_filtered = df.filter(\n",
    "    (pl.col('comm') == 'page_fault') & \n",
    "    (pl.col('is_major') == True)\n",
    ")\n",
    "print(f\"Major faults for page_fault app: {len(df_filtered)}\")\n",
    "\n",
    "major_summary = df.filter(pl.col('is_major')).group_by('comm').len()\n",
    "print(major_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6f636-6e03-4a04-bc5d-6b4cff6c9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = Collector(\"./config/raw_overrides.yaml\")\n",
    "# This creates a raw collector, I suggest looking into this file to learn more\n",
    "\n",
    "w = open(\"hello.txt\", \"bw\")\n",
    "collect.start_collection(logfile=w)\n",
    "print(\"Collection has started\")\n",
    "# Start collection\n",
    "\n",
    "f = open(\"blah.txt\", \"w\")\n",
    "bench_test = subprocess.Popen([\"cat\", \"defaults.yaml\"], stdout=f)\n",
    "bench_test.wait()\n",
    "# Run benchmark application\n",
    "\n",
    "# Run a program that causes page faults\n",
    "!python -c \"import numpy as np; a = np.zeros((1000, 1000, 100))\" & echo $!\n",
    "!ps -a\n",
    "\n",
    "print(\"Exit application\")\n",
    "raw_coll_info = collect.stop_collection()\n",
    "print(raw_coll_info)\n",
    "# Stop the Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68efc94-d113-4ea6-afd5-098f2f509ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze page fault results here\n",
    "import polars as pl\n",
    "df = pl.read_parquet(raw_coll_info[\"page_fault\"])\n",
    "print(df.head())\n",
    "print(f\"Total faults: {len(df)}\")\n",
    "print(f\"Major faults: {df.filter(pl.col('is_major')).height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b719918-a008-4385-8c0b-59fd99faa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = Collector(\"./config/start_overrides.yaml\")\n",
    "# This is a simple redis benchmark config\n",
    "\n",
    "w = open(\"hello.txt\", \"bw\")\n",
    "collect.start_collection(logfile=w)\n",
    "# Start collection\n",
    "\n",
    "start_coll_info = collect.wait()\n",
    "#Wait for collector to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77c577-d4bd-4c97-86dd-f69388c796de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_coll_info.keys())\n",
    "print(start_coll_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb351660-009f-4c1f-9cd6-8d385159c023",
   "metadata": {},
   "source": [
    "Now let's try to examine some of the system information from this.\n",
    "I use polars, you can use whatever you like as far as data frames go, so long as they can read parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f164d85-48c6-4a1e-992c-28c17ac35fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_parquet(raw_coll_info[\"process_trace\"])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9258bc08-a748-4cb4-9ad8-466aeec46b6b",
   "metadata": {},
   "source": [
    "As a very easy excercise we could filter out the processes that are created during the life of the program and give them the name of the last exec call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b0c2e-8e9f-4422-b729-be8c0a2ae1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_parquet(raw_coll_info[\"process_trace\"])\n",
    "\n",
    "def filter_process_trace(process_trace_df: pl.DataFrame) -> pl.DataFrame :\n",
    "    df = process_trace_df\n",
    "    # Filter just the processes\n",
    "    df = df.filter(pl.col(\"tgid\") == pl.col(\"pid\")).drop(\"collection_id\")\n",
    "\n",
    "    # Find the last name of each process\n",
    "    start_df = df.sort(pl.col(\"ts_ns\"), descending = True)\n",
    "    helper_dict = {}\n",
    "    for row in start_df.iter_rows():\n",
    "        pid = row[0]\n",
    "        comm = row[3]\n",
    "        if pid in helper_dict.keys() or comm == \"\": \n",
    "            continue\n",
    "        helper_dict[pid] = comm\n",
    "\n",
    "    # Separate the start and end\n",
    "    full_df = start_df.with_columns(pl.col(\"pid\").map_elements(lambda x : helper_dict.get(x, \"\"), return_dtype=str).alias(\"full_name\"))\n",
    "    full_df = full_df.drop([\"tgid\", \"name\"])\n",
    "    start_df = full_df.filter(pl.col(\"cap_type\") == \"start\").rename({\"ts_ns\": \"start_ns\"}).drop(\"cap_type\")\n",
    "    end_df = full_df.filter(pl.col(\"cap_type\") == \"end\").rename({\"ts_ns\": \"end_ns\"}).drop([\"cap_type\", \"full_name\"])\n",
    "\n",
    "    # Join them to get the process table\n",
    "    return start_df.join(end_df, \"pid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247cd05-d451-48b0-a83f-a2404f9f5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_process_trace(pl.read_parquet(raw_coll_info[\"process_trace\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705e626-63c6-42da-9d6e-4a9f916e8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_process_trace(pl.read_parquet(start_coll_info[\"process_trace\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fa599-56a0-4773-be03-7629f6d337a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def filter_process_trace(process_trace_df: pl.DataFrame) -> pl.DataFrame :\n",
    "    df = process_trace_df\n",
    "    # Filter just the processes\n",
    "    df = df.filter(pl.col(\"tgid\") == pl.col(\"pid\")).drop(\"collection_id\")\n",
    "\n",
    "    # Find the last name of each process\n",
    "    start_df = df.sort(pl.col(\"ts_ns\"), descending = True)\n",
    "    helper_dict = {}\n",
    "    for row in start_df.iter_rows():\n",
    "        pid = row[0]\n",
    "        comm = row[3]\n",
    "        if pid in helper_dict.keys() or comm == \"\": \n",
    "            continue\n",
    "        helper_dict[pid] = comm\n",
    "\n",
    "    # Separate the start and end\n",
    "    full_df = start_df.with_columns(pl.col(\"pid\").map_elements(lambda x : helper_dict.get(x, \"\"), return_dtype=str).alias(\"full_name\"))\n",
    "    full_df = full_df.drop([\"tgid\", \"name\"])\n",
    "    start_df = full_df.filter(pl.col(\"cap_type\") == \"start\").rename({\"ts_ns\": \"start_ns\"}).drop(\"cap_type\")\n",
    "    end_df = full_df.filter(pl.col(\"cap_type\") == \"end\").rename({\"ts_ns\": \"end_ns\"}).drop([\"cap_type\", \"full_name\"])\n",
    "\n",
    "    # Join them to get the process table\n",
    "    combined_df = start_df.join(end_df, \"pid\")\n",
    "    return combined_df.with_columns((pl.col(\"end_ns\") - pl.col(\"start_ns\")).alias(\"duration\"))\n",
    "\n",
    "def process_trace_start_end_ts(process_trace_df: pl.DataFrame, proc_name: str, index: int) ->(int, int, int):\n",
    "    trace_df = filter_process_trace(process_trace_df).sort(pl.col(\"start_ns\"))\n",
    "    df = trace_df.filter(pl.col(\"full_name\") == proc_name)\n",
    "    print(df)\n",
    "    df = df[index]\n",
    "    pid = df[\"pid\"][0]\n",
    "    start_ns = df[\"start_ns\"][0]\n",
    "    end_ns = df[\"end_ns\"][0]\n",
    "    return pid, start_ns, end_ns\n",
    "\n",
    "def clean_rss_pid(rss_df: pl.DataFrame, pid: int) -> pl.DataFrame:\n",
    "    df = rss_df.drop([\"pid\", \"collection_id\"]).sort(pl.col(\"ts_ns\"))\n",
    "    df = df.filter(pl.col(\"tgid\") == pid)\n",
    "    df = df.with_columns(pl.when(pl.col(\"member\") == \"MM_FILEPAGES\")\n",
    "                     .then(pl.col(\"count\"))\n",
    "                     .otherwise(None)\n",
    "                     .fill_null(strategy=\"forward\")\n",
    "                     .fill_null(strategy=\"backward\")\n",
    "                     .alias(\"file\"))\n",
    "    df = df.with_columns(pl.when(pl.col(\"member\") == \"MM_ANONPAGES\")\n",
    "                     .then(pl.col(\"count\"))\n",
    "                     .otherwise(None)\n",
    "                     .fill_null(strategy=\"forward\")\n",
    "                     .fill_null(strategy=\"backward\")\n",
    "                     .alias(\"anon\"))\n",
    "    df = df.with_columns(pl.when(pl.col(\"member\") == \"MM_SWAPENTS\")\n",
    "                     .then(pl.col(\"count\"))\n",
    "                     .otherwise(None)\n",
    "                     .fill_null(strategy=\"forward\")\n",
    "                     .fill_null(strategy=\"backward\")\n",
    "                     .alias(\"swap\"))\n",
    "    df = df.drop([\"member\", \"count\"])\n",
    "    zero_df = pl.DataFrame({\"tgid\": pid, \"ts_ns\" : -1, \"file\" : 0, \"anon\": 0, \"swap\": 0})\n",
    "    df = pl.concat([df, zero_df]).sort(\"ts_ns\")\n",
    "    df = df.fill_null(strategy=\"forward\")\n",
    "    df = df.filter(pl.col(\"ts_ns\") >= 0)\n",
    "    df = df.with_columns((pl.col(\"file\") + pl.col(\"anon\") + pl.col(\"swap\")).alias(\"count\"))\n",
    "    df = df.drop([\"file\", \"anon\", \"swap\"])\n",
    "    return df\n",
    "\n",
    "def filter_rss_with_ts(rss_trace_df: pl.DataFrame, start: int, end: int):\n",
    "    print(start, end)\n",
    "    new_frame_dict = {}\n",
    "    for column_name in rss_trace_df.columns:\n",
    "        new_frame_dict[column_name] = [None, None]\n",
    "    new_frame_dict[\"ts_ns\"] = [start, end]\n",
    "    df = rss_trace_df.vstack(pl.DataFrame(new_frame_dict))\n",
    "    df = df.sort(pl.col(\"ts_ns\")).fill_null(strategy=\"forward\").fill_null(strategy=\"backward\")\n",
    "    return df.filter(pl.col(\"ts_ns\").is_between(start, end, closed='both'))\n",
    "\n",
    "def get_proper_rss(proc_path: Path, rss_path: Path, rss_name: str, rss_ind: int, runner_name: str, runner_ind: int, tag:str):\n",
    "    proc_trace_df = pl.read_parquet(proc_path)\n",
    "    rss_df = pl.read_parquet(rss_path)\n",
    "\n",
    "    _, start, end = process_trace_start_end_ts(proc_trace_df, runner_name, runner_ind)\n",
    "    pid, _, _ = process_trace_start_end_ts(proc_trace_df, rss_name, rss_ind)\n",
    "    clean_rss_df = filter_rss_with_ts(clean_rss_pid(rss_df, pid), start, end)\n",
    "    return clean_rss_df.with_columns((pl.col(\"ts_ns\") - pl.min(\"ts_ns\")).alias(\"norm_ts_ns\")).with_columns(pl.lit(tag).alias('policy'))\n",
    "\n",
    "from pathlib import Path\n",
    "from plotnine import ggplot, aes, geom_line, geom_point, labs\n",
    "\n",
    "def create_graph(inputs: [(str, dict[str, Path])], proc_tag: str, proc_ind: int, time_proc_tag: str, time_proc_index: int, title: str) -> None:\n",
    "    df = pl.DataFrame()\n",
    "    for (tag, filedict) in inputs:\n",
    "        append_df = get_proper_rss(filedict[\"process_trace\"],\n",
    "                                   filedict[\"mm_rss_stat\"],\n",
    "                                   proc_tag, proc_ind,\n",
    "                                   time_proc_tag,\n",
    "                                   time_proc_index,\n",
    "                                   tag).drop([\"tgid\", \"ts_ns\"])\n",
    "        df = pl.concat([df, append_df])\n",
    "    df = df.with_columns((pl.col(\"norm_ts_ns\") / (10**9)/ 60).alias(\"norm_ts_mins\"))\n",
    "    plt0 = (ggplot(df)\n",
    "            + aes(\"norm_ts_mins\", y=\"count\", colour=\"policy\")\n",
    "            + geom_point()\n",
    "            + geom_line()\n",
    "            + labs(x=\"Time (mins)\",\n",
    "                   y=\"4kB Pages\",\n",
    "                   title=title)\n",
    "           )\n",
    "    return plt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94874478-33b2-4c67-989c-dc98cfc529f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = Collector(\"./config/redis_never.yaml\")\n",
    "collect.start_collection(None)\n",
    "redis_never_info = collect.wait()\n",
    "collect = Collector(\"./config/redis_madvise.yaml\")\n",
    "collect.start_collection(None)\n",
    "redis_madvise_info = collect.wait()\n",
    "collect = Collector(\"./config/redis_always.yaml\")\n",
    "collect.start_collection(None)\n",
    "redis_always_info = collect.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcfd78-4c61-4c00-b3a2-5fb4d8a415cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "plt = create_graph([(\"4k\", redis_never_info),\n",
    "                    (\"madvise\", redis_madvise_info),\n",
    "                    (\"thp\", redis_always_info)],\n",
    "                   \"redis-server\", 0, \n",
    "                   \"redis-server\", 0,\n",
    "                   \"Redis driven by YCSB with Insertions and Deletes using Jemalloc\")\n",
    "plt.save(\"deletes-redis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28cd4d-c553-467e-83e3-85cf2a64bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"./deletes-redis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad914c-bd29-4e80-a5c9-f37cb88f14d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
